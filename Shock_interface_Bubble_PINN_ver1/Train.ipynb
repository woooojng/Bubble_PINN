{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d8322d-5a4c-4bc7-9219-82163d85a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Only on the Initial Condition\n",
      "Iteration: 1000 Initial Condition Loss: tensor(0.1349)\n",
      "Iteration: 2000 Initial Condition Loss: tensor(0.0370)\n",
      "Iteration: 3000 Initial Condition Loss: tensor(0.0257)\n",
      "Iteration: 4000 Initial Condition Loss: tensor(0.0147)\n",
      "Iteration: 5000 Initial Condition Loss: tensor(0.0104)\n",
      "Iteration: 6000 Initial Condition Loss: tensor(0.0078)\n",
      "Iteration: 7000 Initial Condition Loss: tensor(0.0051)\n",
      "Iteration: 8000 Initial Condition Loss: tensor(0.0061)\n",
      "Iteration: 9000 Initial Condition Loss: tensor(0.0038)\n",
      "Iteration: 10000 Initial Condition Loss: tensor(0.0037)\n",
      "Iteration: 11000 Initial Condition Loss: tensor(0.0028)\n",
      "Iteration: 12000 Initial Condition Loss: tensor(0.0031)\n",
      "Iteration: 13000 Initial Condition Loss: tensor(0.0022)\n",
      "Iteration: 14000 Initial Condition Loss: tensor(0.0018)\n",
      "Iteration: 15000 Initial Condition Loss: tensor(0.0031)\n",
      "Iteration: 16000 Initial Condition Loss: tensor(0.0027)\n",
      "Iteration: 17000 Initial Condition Loss: tensor(0.0024)\n",
      "Iteration: 18000 Initial Condition Loss: tensor(0.0022)\n",
      "Iteration: 19000 Initial Condition Loss: tensor(0.0018)\n",
      "Iteration: 20000 Initial Condition Loss: tensor(0.0015)\n",
      "Iteration: 21000 Initial Condition Loss: tensor(0.0020)\n",
      "Iteration: 22000 Initial Condition Loss: tensor(0.0017)\n",
      "Iteration: 23000 Initial Condition Loss: tensor(0.0014)\n",
      "Iteration: 24000 Initial Condition Loss: tensor(0.0030)\n",
      "Iteration: 25000 Initial Condition Loss: tensor(0.0023)\n",
      "Iteration: 26000 Initial Condition Loss: tensor(0.0021)\n",
      "Iteration: 27000 Initial Condition Loss: tensor(0.0020)\n",
      "Iteration: 28000 Initial Condition Loss: tensor(0.0019)\n",
      "Iteration: 29000 Initial Condition Loss: tensor(0.0017)\n",
      "Iteration: 30000 Initial Condition Loss: tensor(0.0016)\n",
      "IC Time:\t 176.93385887145996\n",
      "Training PDE\n",
      "Executing Pass 1\n",
      "Current Final Time: 0.01 Current Learning Rate:  0.0001\n",
      "Iteration: 200 \tTotal Loss: tensor(2.6371e+12)\n",
      "IC Loss:  tensor(0.0276) \tBC Loss:  tensor(5.1546e-11) \tNS PDE Loss:  tensor(57.5175) \tMv Bdry Loss:  tensor(2.6371e+12)\n",
      "Iteration: 400 \tTotal Loss: tensor(1.9742e+12)\n",
      "IC Loss:  tensor(0.0276) \tBC Loss:  tensor(6.1070e-11) \tNS PDE Loss:  tensor(57.7409) \tMv Bdry Loss:  tensor(1.9742e+12)\n",
      "Iteration: 600 \tTotal Loss: tensor(1.7611e+12)\n",
      "IC Loss:  tensor(0.0277) \tBC Loss:  tensor(1.4775e-10) \tNS PDE Loss:  tensor(57.8750) \tMv Bdry Loss:  tensor(1.7611e+12)\n",
      "Iteration: 800 \tTotal Loss: tensor(1.5091e+12)\n",
      "IC Loss:  tensor(0.0274) \tBC Loss:  tensor(7.7698e-11) \tNS PDE Loss:  tensor(58.0805) \tMv Bdry Loss:  tensor(1.5091e+12)\n",
      "Iteration: 1000 \tTotal Loss: tensor(1.3584e+12)\n",
      "IC Loss:  tensor(0.0275) \tBC Loss:  tensor(9.9764e-11) \tNS PDE Loss:  tensor(58.2939) \tMv Bdry Loss:  tensor(1.3584e+12)\n",
      "Iteration: 1200 \tTotal Loss: tensor(1.3041e+12)\n",
      "IC Loss:  tensor(0.0276) \tBC Loss:  tensor(9.3839e-11) \tNS PDE Loss:  tensor(58.3659) \tMv Bdry Loss:  tensor(1.3041e+12)\n",
      "Iteration: 1400 \tTotal Loss: tensor(1.2549e+12)\n",
      "IC Loss:  tensor(0.0277) \tBC Loss:  tensor(1.0293e-10) \tNS PDE Loss:  tensor(58.4264) \tMv Bdry Loss:  tensor(1.2549e+12)\n",
      "Iteration: 1600 \tTotal Loss: tensor(1.2069e+12)\n",
      "IC Loss:  tensor(0.0278) \tBC Loss:  tensor(1.3863e-10) \tNS PDE Loss:  tensor(58.4890) \tMv Bdry Loss:  tensor(1.2069e+12)\n",
      "Iteration: 1800 \tTotal Loss: tensor(1.1627e+12)\n",
      "IC Loss:  tensor(0.0279) \tBC Loss:  tensor(1.0334e-10) \tNS PDE Loss:  tensor(58.5446) \tMv Bdry Loss:  tensor(1.1627e+12)\n",
      "Iteration: 2000 \tTotal Loss: tensor(1.2216e+12)\n",
      "IC Loss:  tensor(0.0281) \tBC Loss:  tensor(8.9336e-11) \tNS PDE Loss:  tensor(58.6172) \tMv Bdry Loss:  tensor(1.2216e+12)\n",
      "Iteration: 2200 \tTotal Loss: tensor(1.0675e+12)\n",
      "IC Loss:  tensor(0.0282) \tBC Loss:  tensor(9.1027e-11) \tNS PDE Loss:  tensor(58.6469) \tMv Bdry Loss:  tensor(1.0675e+12)\n",
      "Iteration: 2400 \tTotal Loss: tensor(1.0157e+12)\n",
      "IC Loss:  tensor(0.0283) \tBC Loss:  tensor(1.0364e-10) \tNS PDE Loss:  tensor(58.7101) \tMv Bdry Loss:  tensor(1.0156e+12)\n",
      "Iteration: 2600 \tTotal Loss: tensor(9.6266e+11)\n",
      "IC Loss:  tensor(0.0285) \tBC Loss:  tensor(1.1801e-10) \tNS PDE Loss:  tensor(58.7816) \tMv Bdry Loss:  tensor(9.6265e+11)\n",
      "Iteration: 2800 \tTotal Loss: tensor(9.0957e+11)\n",
      "IC Loss:  tensor(0.0287) \tBC Loss:  tensor(1.1833e-10) \tNS PDE Loss:  tensor(58.8496) \tMv Bdry Loss:  tensor(9.0956e+11)\n",
      "Iteration: 3000 \tTotal Loss: tensor(9.2330e+11)\n",
      "IC Loss:  tensor(0.0289) \tBC Loss:  tensor(1.3877e-10) \tNS PDE Loss:  tensor(58.9387) \tMv Bdry Loss:  tensor(9.2330e+11)\n",
      "Iteration: 3200 \tTotal Loss: tensor(7.9935e+11)\n",
      "IC Loss:  tensor(0.0290) \tBC Loss:  tensor(1.5514e-10) \tNS PDE Loss:  tensor(58.9821) \tMv Bdry Loss:  tensor(7.9935e+11)\n",
      "Iteration: 3400 \tTotal Loss: tensor(7.4254e+11)\n",
      "IC Loss:  tensor(0.0292) \tBC Loss:  tensor(1.0503e-10) \tNS PDE Loss:  tensor(59.0597) \tMv Bdry Loss:  tensor(7.4253e+11)\n",
      "Iteration: 3600 \tTotal Loss: tensor(6.8415e+11)\n",
      "IC Loss:  tensor(0.0294) \tBC Loss:  tensor(1.4402e-10) \tNS PDE Loss:  tensor(59.1468) \tMv Bdry Loss:  tensor(6.8414e+11)\n",
      "Iteration: 3800 \tTotal Loss: tensor(6.2516e+11)\n",
      "IC Loss:  tensor(0.0296) \tBC Loss:  tensor(9.1925e-11) \tNS PDE Loss:  tensor(59.2584) \tMv Bdry Loss:  tensor(6.2516e+11)\n",
      "Iteration: 4000 \tTotal Loss: tensor(6.0437e+11)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(2.0256e-11) \tNS PDE Loss:  tensor(59.4103) \tMv Bdry Loss:  tensor(6.0436e+11)\n",
      "Iteration: 4200 \tTotal Loss: tensor(5.1645e+11)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.9719e-10) \tNS PDE Loss:  tensor(59.4995) \tMv Bdry Loss:  tensor(5.1645e+11)\n",
      "Iteration: 4400 \tTotal Loss: tensor(4.7313e+11)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(6.3940e-10) \tNS PDE Loss:  tensor(59.6014) \tMv Bdry Loss:  tensor(4.7312e+11)\n",
      "Iteration: 4600 \tTotal Loss: tensor(4.3431e+11)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(5.3459e-10) \tNS PDE Loss:  tensor(59.7089) \tMv Bdry Loss:  tensor(4.3431e+11)\n",
      "Iteration: 4800 \tTotal Loss: tensor(3.9734e+11)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.2353e-10) \tNS PDE Loss:  tensor(59.8408) \tMv Bdry Loss:  tensor(3.9733e+11)\n",
      "Iteration: 5000 \tTotal Loss: tensor(3.8295e+11)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(5.1277e-10) \tNS PDE Loss:  tensor(60.0389) \tMv Bdry Loss:  tensor(3.8294e+11)\n",
      "Iteration: 5200 \tTotal Loss: tensor(3.6235e+11)\n",
      "IC Loss:  tensor(0.0295) \tBC Loss:  tensor(2.7246e-10) \tNS PDE Loss:  tensor(60.3514) \tMv Bdry Loss:  tensor(3.6234e+11)\n",
      "Iteration: 5400 \tTotal Loss: tensor(2.8937e+11)\n",
      "IC Loss:  tensor(0.0293) \tBC Loss:  tensor(3.0339e-09) \tNS PDE Loss:  tensor(60.3303) \tMv Bdry Loss:  tensor(2.8937e+11)\n",
      "Iteration: 5600 \tTotal Loss: tensor(2.4584e+11)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(8.1092e-09) \tNS PDE Loss:  tensor(60.2497) \tMv Bdry Loss:  tensor(2.4584e+11)\n",
      "Iteration: 5800 \tTotal Loss: tensor(2.2920e+11)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(1.6704e-10) \tNS PDE Loss:  tensor(60.1707) \tMv Bdry Loss:  tensor(2.2919e+11)\n",
      "Iteration: 6000 \tTotal Loss: tensor(2.2321e+11)\n",
      "IC Loss:  tensor(0.0307) \tBC Loss:  tensor(1.1533e-10) \tNS PDE Loss:  tensor(60.1010) \tMv Bdry Loss:  tensor(2.2321e+11)\n",
      "Iteration: 6200 \tTotal Loss: tensor(2.1040e+11)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(3.1796e-10) \tNS PDE Loss:  tensor(60.0789) \tMv Bdry Loss:  tensor(2.1040e+11)\n",
      "Iteration: 6400 \tTotal Loss: tensor(2.0656e+11)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(3.9250e-10) \tNS PDE Loss:  tensor(60.0704) \tMv Bdry Loss:  tensor(2.0655e+11)\n",
      "Iteration: 6600 \tTotal Loss: tensor(2.0368e+11)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(1.3490e-09) \tNS PDE Loss:  tensor(60.0758) \tMv Bdry Loss:  tensor(2.0367e+11)\n",
      "Iteration: 6800 \tTotal Loss: tensor(2.0078e+11)\n",
      "IC Loss:  tensor(0.0307) \tBC Loss:  tensor(1.5103e-10) \tNS PDE Loss:  tensor(60.0893) \tMv Bdry Loss:  tensor(2.0077e+11)\n",
      "Iteration: 7000 \tTotal Loss: tensor(2.0247e+11)\n",
      "IC Loss:  tensor(0.0307) \tBC Loss:  tensor(3.5853e-11) \tNS PDE Loss:  tensor(60.1060) \tMv Bdry Loss:  tensor(2.0247e+11)\n",
      "Iteration: 7200 \tTotal Loss: tensor(1.9514e+11)\n",
      "IC Loss:  tensor(0.0306) \tBC Loss:  tensor(3.8689e-11) \tNS PDE Loss:  tensor(60.1246) \tMv Bdry Loss:  tensor(1.9513e+11)\n",
      "Iteration: 7400 \tTotal Loss: tensor(1.9227e+11)\n",
      "IC Loss:  tensor(0.0305) \tBC Loss:  tensor(1.0133e-11) \tNS PDE Loss:  tensor(60.1324) \tMv Bdry Loss:  tensor(1.9226e+11)\n",
      "Iteration: 7600 \tTotal Loss: tensor(1.5816e+11)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(1.8865e-09) \tNS PDE Loss:  tensor(60.1776) \tMv Bdry Loss:  tensor(1.5815e+11)\n",
      "Iteration: 7800 \tTotal Loss: tensor(1.3900e+11)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(1.4726e-08) \tNS PDE Loss:  tensor(60.1308) \tMv Bdry Loss:  tensor(1.3900e+11)\n",
      "Iteration: 8000 \tTotal Loss: tensor(1.1724e+11)\n",
      "IC Loss:  tensor(0.0306) \tBC Loss:  tensor(2.3654e-08) \tNS PDE Loss:  tensor(60.0850) \tMv Bdry Loss:  tensor(1.1723e+11)\n",
      "Iteration: 8200 \tTotal Loss: tensor(1.1011e+11)\n",
      "IC Loss:  tensor(0.0306) \tBC Loss:  tensor(1.8309e-08) \tNS PDE Loss:  tensor(60.0483) \tMv Bdry Loss:  tensor(1.1011e+11)\n",
      "Iteration: 8400 \tTotal Loss: tensor(1.0442e+11)\n",
      "IC Loss:  tensor(0.0306) \tBC Loss:  tensor(1.4280e-08) \tNS PDE Loss:  tensor(60.0130) \tMv Bdry Loss:  tensor(1.0442e+11)\n",
      "Iteration: 8600 \tTotal Loss: tensor(9.9453e+10)\n",
      "IC Loss:  tensor(0.0305) \tBC Loss:  tensor(7.4535e-09) \tNS PDE Loss:  tensor(59.9816) \tMv Bdry Loss:  tensor(9.9447e+10)\n",
      "Iteration: 8800 \tTotal Loss: tensor(9.4868e+10)\n",
      "IC Loss:  tensor(0.0306) \tBC Loss:  tensor(5.0004e-09) \tNS PDE Loss:  tensor(59.9388) \tMv Bdry Loss:  tensor(9.4862e+10)\n",
      "Iteration: 9000 \tTotal Loss: tensor(1.0004e+11)\n",
      "IC Loss:  tensor(0.0305) \tBC Loss:  tensor(1.0945e-08) \tNS PDE Loss:  tensor(59.8940) \tMv Bdry Loss:  tensor(1.0003e+11)\n",
      "Iteration: 9200 \tTotal Loss: tensor(8.6586e+10)\n",
      "IC Loss:  tensor(0.0305) \tBC Loss:  tensor(4.4843e-11) \tNS PDE Loss:  tensor(59.8473) \tMv Bdry Loss:  tensor(8.6580e+10)\n",
      "Iteration: 9400 \tTotal Loss: tensor(9.3321e+10)\n",
      "IC Loss:  tensor(0.0308) \tBC Loss:  tensor(3.5093e-10) \tNS PDE Loss:  tensor(59.8015) \tMv Bdry Loss:  tensor(9.3315e+10)\n",
      "Iteration: 9600 \tTotal Loss: tensor(8.9945e+10)\n",
      "IC Loss:  tensor(0.0309) \tBC Loss:  tensor(6.9450e-10) \tNS PDE Loss:  tensor(59.7090) \tMv Bdry Loss:  tensor(8.9939e+10)\n",
      "Iteration: 9800 \tTotal Loss: tensor(6.5368e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(4.4860e-09) \tNS PDE Loss:  tensor(59.5140) \tMv Bdry Loss:  tensor(6.5362e+10)\n",
      "Iteration: 10000 \tTotal Loss: tensor(6.7753e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(2.2651e-09) \tNS PDE Loss:  tensor(59.4766) \tMv Bdry Loss:  tensor(6.7747e+10)\n",
      "Iteration: 10200 \tTotal Loss: tensor(5.5077e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(4.0227e-07) \tNS PDE Loss:  tensor(59.3030) \tMv Bdry Loss:  tensor(5.5071e+10)\n",
      "Iteration: 10400 \tTotal Loss: tensor(4.5962e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.7462e-07) \tNS PDE Loss:  tensor(59.2606) \tMv Bdry Loss:  tensor(4.5956e+10)\n",
      "Iteration: 10600 \tTotal Loss: tensor(8.2218e+10)\n",
      "IC Loss:  tensor(0.0319) \tBC Loss:  tensor(1.5626e-07) \tNS PDE Loss:  tensor(59.2466) \tMv Bdry Loss:  tensor(8.2213e+10)\n",
      "Iteration: 10800 \tTotal Loss: tensor(5.8093e+10)\n",
      "IC Loss:  tensor(0.0319) \tBC Loss:  tensor(7.0741e-08) \tNS PDE Loss:  tensor(59.1361) \tMv Bdry Loss:  tensor(5.8087e+10)\n",
      "Iteration: 11000 \tTotal Loss: tensor(5.6029e+10)\n",
      "IC Loss:  tensor(0.0319) \tBC Loss:  tensor(3.3649e-08) \tNS PDE Loss:  tensor(59.1301) \tMv Bdry Loss:  tensor(5.6024e+10)\n",
      "Iteration: 11200 \tTotal Loss: tensor(5.1440e+10)\n",
      "IC Loss:  tensor(0.0320) \tBC Loss:  tensor(1.6098e-08) \tNS PDE Loss:  tensor(59.0134) \tMv Bdry Loss:  tensor(5.1434e+10)\n",
      "Iteration: 11400 \tTotal Loss: tensor(5.2155e+10)\n",
      "IC Loss:  tensor(0.0320) \tBC Loss:  tensor(2.1253e-09) \tNS PDE Loss:  tensor(58.8874) \tMv Bdry Loss:  tensor(5.2149e+10)\n",
      "Iteration: 11600 \tTotal Loss: tensor(4.9795e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(5.0427e-10) \tNS PDE Loss:  tensor(58.7408) \tMv Bdry Loss:  tensor(4.9789e+10)\n",
      "Iteration: 11800 \tTotal Loss: tensor(3.2664e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(1.3025e-08) \tNS PDE Loss:  tensor(58.6565) \tMv Bdry Loss:  tensor(3.2658e+10)\n",
      "Iteration: 12000 \tTotal Loss: tensor(3.2826e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(9.7817e-09) \tNS PDE Loss:  tensor(58.6355) \tMv Bdry Loss:  tensor(3.2820e+10)\n",
      "Iteration: 12200 \tTotal Loss: tensor(3.2212e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(1.0475e-08) \tNS PDE Loss:  tensor(58.6193) \tMv Bdry Loss:  tensor(3.2206e+10)\n",
      "Iteration: 12400 \tTotal Loss: tensor(3.2219e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(5.9728e-09) \tNS PDE Loss:  tensor(58.6070) \tMv Bdry Loss:  tensor(3.2213e+10)\n",
      "Iteration: 12600 \tTotal Loss: tensor(3.2195e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(6.5359e-09) \tNS PDE Loss:  tensor(58.5961) \tMv Bdry Loss:  tensor(3.2189e+10)\n",
      "Iteration: 12800 \tTotal Loss: tensor(3.2213e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(5.1354e-09) \tNS PDE Loss:  tensor(58.5847) \tMv Bdry Loss:  tensor(3.2207e+10)\n",
      "Iteration: 13000 \tTotal Loss: tensor(3.2392e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.1378e-09) \tNS PDE Loss:  tensor(58.5748) \tMv Bdry Loss:  tensor(3.2386e+10)\n",
      "Iteration: 13200 \tTotal Loss: tensor(3.1275e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(1.0624e-09) \tNS PDE Loss:  tensor(58.5647) \tMv Bdry Loss:  tensor(3.1270e+10)\n",
      "Iteration: 13400 \tTotal Loss: tensor(3.1731e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(1.5105e-09) \tNS PDE Loss:  tensor(58.5539) \tMv Bdry Loss:  tensor(3.1725e+10)\n",
      "Iteration: 13600 \tTotal Loss: tensor(3.1795e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(7.1081e-10) \tNS PDE Loss:  tensor(58.5443) \tMv Bdry Loss:  tensor(3.1789e+10)\n",
      "Iteration: 13800 \tTotal Loss: tensor(3.1501e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(7.1442e-10) \tNS PDE Loss:  tensor(58.5337) \tMv Bdry Loss:  tensor(3.1495e+10)\n",
      "Iteration: 14000 \tTotal Loss: tensor(3.2378e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(1.5185e-09) \tNS PDE Loss:  tensor(58.5289) \tMv Bdry Loss:  tensor(3.2373e+10)\n",
      "Iteration: 14200 \tTotal Loss: tensor(2.9988e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.4357e-09) \tNS PDE Loss:  tensor(58.5215) \tMv Bdry Loss:  tensor(2.9983e+10)\n",
      "Iteration: 14400 \tTotal Loss: tensor(3.0612e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(1.0244e-09) \tNS PDE Loss:  tensor(58.5126) \tMv Bdry Loss:  tensor(3.0606e+10)\n",
      "Iteration: 14600 \tTotal Loss: tensor(3.1837e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.1006e-09) \tNS PDE Loss:  tensor(58.5017) \tMv Bdry Loss:  tensor(3.1831e+10)\n",
      "Iteration: 14800 \tTotal Loss: tensor(3.0735e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(2.3021e-09) \tNS PDE Loss:  tensor(58.5005) \tMv Bdry Loss:  tensor(3.0730e+10)\n",
      "Iteration: 15000 \tTotal Loss: tensor(3.1620e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.0937e-09) \tNS PDE Loss:  tensor(58.4833) \tMv Bdry Loss:  tensor(3.1614e+10)\n",
      "Iteration: 15200 \tTotal Loss: tensor(3.2066e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(1.0419e-09) \tNS PDE Loss:  tensor(58.4716) \tMv Bdry Loss:  tensor(3.2060e+10)\n",
      "Iteration: 15400 \tTotal Loss: tensor(2.8573e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(5.1641e-11) \tNS PDE Loss:  tensor(58.4666) \tMv Bdry Loss:  tensor(2.8567e+10)\n",
      "Iteration: 15600 \tTotal Loss: tensor(2.8679e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(1.3375e-10) \tNS PDE Loss:  tensor(58.4586) \tMv Bdry Loss:  tensor(2.8673e+10)\n",
      "Iteration: 15800 \tTotal Loss: tensor(2.8918e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(3.6133e-10) \tNS PDE Loss:  tensor(58.4498) \tMv Bdry Loss:  tensor(2.8913e+10)\n",
      "Iteration: 16000 \tTotal Loss: tensor(2.9637e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(1.1346e-09) \tNS PDE Loss:  tensor(58.4427) \tMv Bdry Loss:  tensor(2.9631e+10)\n",
      "Iteration: 16200 \tTotal Loss: tensor(2.9369e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(1.0693e-09) \tNS PDE Loss:  tensor(58.4345) \tMv Bdry Loss:  tensor(2.9363e+10)\n",
      "Iteration: 16400 \tTotal Loss: tensor(3.0184e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(8.2159e-10) \tNS PDE Loss:  tensor(58.4273) \tMv Bdry Loss:  tensor(3.0178e+10)\n",
      "Iteration: 16600 \tTotal Loss: tensor(2.8277e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(1.4145e-09) \tNS PDE Loss:  tensor(58.4229) \tMv Bdry Loss:  tensor(2.8271e+10)\n",
      "Iteration: 16800 \tTotal Loss: tensor(2.8516e+10)\n",
      "IC Loss:  tensor(0.0314) \tBC Loss:  tensor(8.1781e-10) \tNS PDE Loss:  tensor(58.4123) \tMv Bdry Loss:  tensor(2.8510e+10)\n",
      "Iteration: 17000 \tTotal Loss: tensor(3.0900e+10)\n",
      "IC Loss:  tensor(0.0314) \tBC Loss:  tensor(1.1843e-09) \tNS PDE Loss:  tensor(58.4060) \tMv Bdry Loss:  tensor(3.0894e+10)\n",
      "Iteration: 17200 \tTotal Loss: tensor(5.3966e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(2.4463e-09) \tNS PDE Loss:  tensor(58.4769) \tMv Bdry Loss:  tensor(5.3960e+10)\n",
      "Iteration: 17400 \tTotal Loss: tensor(2.6084e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(9.4803e-09) \tNS PDE Loss:  tensor(58.4394) \tMv Bdry Loss:  tensor(2.6078e+10)\n",
      "Iteration: 17600 \tTotal Loss: tensor(2.3052e+10)\n",
      "IC Loss:  tensor(0.0318) \tBC Loss:  tensor(1.3859e-08) \tNS PDE Loss:  tensor(58.4243) \tMv Bdry Loss:  tensor(2.3046e+10)\n",
      "Iteration: 17800 \tTotal Loss: tensor(3.1231e+10)\n",
      "IC Loss:  tensor(0.0317) \tBC Loss:  tensor(1.0976e-08) \tNS PDE Loss:  tensor(58.4507) \tMv Bdry Loss:  tensor(3.1225e+10)\n",
      "Iteration: 18000 \tTotal Loss: tensor(3.8252e+10)\n",
      "IC Loss:  tensor(0.0318) \tBC Loss:  tensor(9.0277e-09) \tNS PDE Loss:  tensor(58.4249) \tMv Bdry Loss:  tensor(3.8246e+10)\n",
      "Iteration: 18200 \tTotal Loss: tensor(5.2385e+10)\n",
      "IC Loss:  tensor(0.0323) \tBC Loss:  tensor(5.2594e-09) \tNS PDE Loss:  tensor(58.3808) \tMv Bdry Loss:  tensor(5.2379e+10)\n",
      "Iteration: 18400 \tTotal Loss: tensor(4.0766e+10)\n",
      "IC Loss:  tensor(0.0314) \tBC Loss:  tensor(5.0620e-09) \tNS PDE Loss:  tensor(58.3608) \tMv Bdry Loss:  tensor(4.0760e+10)\n",
      "Iteration: 18600 \tTotal Loss: tensor(4.3120e+10)\n",
      "IC Loss:  tensor(0.0316) \tBC Loss:  tensor(3.0162e-09) \tNS PDE Loss:  tensor(58.3703) \tMv Bdry Loss:  tensor(4.3114e+10)\n",
      "Iteration: 18800 \tTotal Loss: tensor(2.2750e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(2.4491e-09) \tNS PDE Loss:  tensor(58.3648) \tMv Bdry Loss:  tensor(2.2744e+10)\n",
      "Iteration: 19000 \tTotal Loss: tensor(2.2176e+10)\n",
      "IC Loss:  tensor(0.0315) \tBC Loss:  tensor(4.5702e-10) \tNS PDE Loss:  tensor(58.3497) \tMv Bdry Loss:  tensor(2.2170e+10)\n",
      "Iteration: 19200 \tTotal Loss: tensor(4.0453e+10)\n",
      "IC Loss:  tensor(0.0321) \tBC Loss:  tensor(5.5527e-09) \tNS PDE Loss:  tensor(58.3189) \tMv Bdry Loss:  tensor(4.0447e+10)\n",
      "Iteration: 19400 \tTotal Loss: tensor(1.8585e+10)\n",
      "IC Loss:  tensor(0.0305) \tBC Loss:  tensor(5.3319e-10) \tNS PDE Loss:  tensor(58.4035) \tMv Bdry Loss:  tensor(1.8579e+10)\n",
      "Iteration: 19600 \tTotal Loss: tensor(1.6529e+10)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(4.2944e-10) \tNS PDE Loss:  tensor(58.3630) \tMv Bdry Loss:  tensor(1.6523e+10)\n",
      "Iteration: 19800 \tTotal Loss: tensor(1.4438e+10)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(9.5821e-12) \tNS PDE Loss:  tensor(58.3522) \tMv Bdry Loss:  tensor(1.4432e+10)\n",
      "Iteration: 20000 \tTotal Loss: tensor(1.4382e+10)\n",
      "IC Loss:  tensor(0.0302) \tBC Loss:  tensor(6.4942e-11) \tNS PDE Loss:  tensor(58.3373) \tMv Bdry Loss:  tensor(1.4377e+10)\n",
      "Current Final Time: 0.1 Current Learning Rate:  0.0001\n",
      "Iteration: 200 \tTotal Loss: tensor(8.8388e+10)\n",
      "IC Loss:  tensor(0.0263) \tBC Loss:  tensor(1.6808e-09) \tNS PDE Loss:  tensor(59.1531) \tMv Bdry Loss:  tensor(8.8382e+10)\n",
      "Iteration: 400 \tTotal Loss: tensor(5.4459e+10)\n",
      "IC Loss:  tensor(0.0266) \tBC Loss:  tensor(1.8471e-10) \tNS PDE Loss:  tensor(59.2090) \tMv Bdry Loss:  tensor(5.4453e+10)\n",
      "Iteration: 600 \tTotal Loss: tensor(4.1486e+10)\n",
      "IC Loss:  tensor(0.0269) \tBC Loss:  tensor(7.9843e-10) \tNS PDE Loss:  tensor(59.3064) \tMv Bdry Loss:  tensor(4.1480e+10)\n",
      "Iteration: 800 \tTotal Loss: tensor(3.3442e+10)\n",
      "IC Loss:  tensor(0.0270) \tBC Loss:  tensor(3.6508e-11) \tNS PDE Loss:  tensor(59.3395) \tMv Bdry Loss:  tensor(3.3436e+10)\n",
      "Iteration: 1000 \tTotal Loss: tensor(2.8875e+10)\n",
      "IC Loss:  tensor(0.0272) \tBC Loss:  tensor(3.7209e-11) \tNS PDE Loss:  tensor(59.3869) \tMv Bdry Loss:  tensor(2.8869e+10)\n",
      "Iteration: 1200 \tTotal Loss: tensor(2.5890e+10)\n",
      "IC Loss:  tensor(0.0273) \tBC Loss:  tensor(1.9276e-11) \tNS PDE Loss:  tensor(59.4454) \tMv Bdry Loss:  tensor(2.5884e+10)\n",
      "Iteration: 1400 \tTotal Loss: tensor(2.3899e+10)\n",
      "IC Loss:  tensor(0.0273) \tBC Loss:  tensor(7.1192e-10) \tNS PDE Loss:  tensor(59.5072) \tMv Bdry Loss:  tensor(2.3893e+10)\n",
      "Iteration: 1600 \tTotal Loss: tensor(2.2535e+10)\n",
      "IC Loss:  tensor(0.0274) \tBC Loss:  tensor(4.0224e-10) \tNS PDE Loss:  tensor(59.5635) \tMv Bdry Loss:  tensor(2.2529e+10)\n",
      "Iteration: 1800 \tTotal Loss: tensor(2.1373e+10)\n",
      "IC Loss:  tensor(0.0274) \tBC Loss:  tensor(2.0247e-10) \tNS PDE Loss:  tensor(59.6222) \tMv Bdry Loss:  tensor(2.1367e+10)\n",
      "Iteration: 2000 \tTotal Loss: tensor(2.0473e+10)\n",
      "IC Loss:  tensor(0.0275) \tBC Loss:  tensor(5.9687e-11) \tNS PDE Loss:  tensor(59.6583) \tMv Bdry Loss:  tensor(2.0467e+10)\n",
      "Iteration: 2200 \tTotal Loss: tensor(1.9555e+10)\n",
      "IC Loss:  tensor(0.0276) \tBC Loss:  tensor(5.1091e-11) \tNS PDE Loss:  tensor(59.6697) \tMv Bdry Loss:  tensor(1.9549e+10)\n",
      "Iteration: 2400 \tTotal Loss: tensor(1.8717e+10)\n",
      "IC Loss:  tensor(0.0276) \tBC Loss:  tensor(3.8435e-12) \tNS PDE Loss:  tensor(59.6812) \tMv Bdry Loss:  tensor(1.8711e+10)\n",
      "Iteration: 2600 \tTotal Loss: tensor(1.7963e+10)\n",
      "IC Loss:  tensor(0.0277) \tBC Loss:  tensor(1.0146e-09) \tNS PDE Loss:  tensor(59.7011) \tMv Bdry Loss:  tensor(1.7957e+10)\n",
      "Iteration: 2800 \tTotal Loss: tensor(1.7110e+10)\n",
      "IC Loss:  tensor(0.0278) \tBC Loss:  tensor(8.4674e-10) \tNS PDE Loss:  tensor(59.7436) \tMv Bdry Loss:  tensor(1.7104e+10)\n",
      "Iteration: 3000 \tTotal Loss: tensor(1.5780e+10)\n",
      "IC Loss:  tensor(0.0278) \tBC Loss:  tensor(5.5000e-10) \tNS PDE Loss:  tensor(59.8905) \tMv Bdry Loss:  tensor(1.5774e+10)\n",
      "Iteration: 3200 \tTotal Loss: tensor(1.4687e+10)\n",
      "IC Loss:  tensor(0.0278) \tBC Loss:  tensor(5.8075e-11) \tNS PDE Loss:  tensor(59.9815) \tMv Bdry Loss:  tensor(1.4681e+10)\n",
      "Iteration: 3400 \tTotal Loss: tensor(1.4064e+10)\n",
      "IC Loss:  tensor(0.0279) \tBC Loss:  tensor(2.5828e-10) \tNS PDE Loss:  tensor(60.0517) \tMv Bdry Loss:  tensor(1.4058e+10)\n",
      "Iteration: 3600 \tTotal Loss: tensor(1.3551e+10)\n",
      "IC Loss:  tensor(0.0279) \tBC Loss:  tensor(5.6889e-12) \tNS PDE Loss:  tensor(60.0960) \tMv Bdry Loss:  tensor(1.3545e+10)\n",
      "Iteration: 3800 \tTotal Loss: tensor(1.3177e+10)\n",
      "IC Loss:  tensor(0.0280) \tBC Loss:  tensor(7.3397e-11) \tNS PDE Loss:  tensor(60.1322) \tMv Bdry Loss:  tensor(1.3171e+10)\n",
      "Iteration: 4000 \tTotal Loss: tensor(1.2930e+10)\n",
      "IC Loss:  tensor(0.0280) \tBC Loss:  tensor(3.1164e-09) \tNS PDE Loss:  tensor(60.1630) \tMv Bdry Loss:  tensor(1.2924e+10)\n",
      "Iteration: 4200 \tTotal Loss: tensor(1.2586e+10)\n",
      "IC Loss:  tensor(0.0280) \tBC Loss:  tensor(5.1696e-10) \tNS PDE Loss:  tensor(60.1838) \tMv Bdry Loss:  tensor(1.2580e+10)\n",
      "Iteration: 4400 \tTotal Loss: tensor(1.2391e+10)\n",
      "IC Loss:  tensor(0.0281) \tBC Loss:  tensor(8.7800e-10) \tNS PDE Loss:  tensor(60.2002) \tMv Bdry Loss:  tensor(1.2385e+10)\n",
      "Iteration: 4600 \tTotal Loss: tensor(1.2200e+10)\n",
      "IC Loss:  tensor(0.0281) \tBC Loss:  tensor(6.1047e-10) \tNS PDE Loss:  tensor(60.2233) \tMv Bdry Loss:  tensor(1.2194e+10)\n",
      "Iteration: 4800 \tTotal Loss: tensor(1.2029e+10)\n",
      "IC Loss:  tensor(0.0282) \tBC Loss:  tensor(2.2495e-11) \tNS PDE Loss:  tensor(60.2492) \tMv Bdry Loss:  tensor(1.2023e+10)\n",
      "Iteration: 5000 \tTotal Loss: tensor(1.1924e+10)\n",
      "IC Loss:  tensor(0.0282) \tBC Loss:  tensor(5.4173e-10) \tNS PDE Loss:  tensor(60.2768) \tMv Bdry Loss:  tensor(1.1918e+10)\n",
      "Iteration: 5200 \tTotal Loss: tensor(1.1691e+10)\n",
      "IC Loss:  tensor(0.0282) \tBC Loss:  tensor(1.1810e-09) \tNS PDE Loss:  tensor(60.3051) \tMv Bdry Loss:  tensor(1.1685e+10)\n",
      "Iteration: 5400 \tTotal Loss: tensor(1.1580e+10)\n",
      "IC Loss:  tensor(0.0283) \tBC Loss:  tensor(2.1519e-10) \tNS PDE Loss:  tensor(60.3286) \tMv Bdry Loss:  tensor(1.1574e+10)\n",
      "Iteration: 5600 \tTotal Loss: tensor(1.1457e+10)\n",
      "IC Loss:  tensor(0.0283) \tBC Loss:  tensor(2.2539e-10) \tNS PDE Loss:  tensor(60.3463) \tMv Bdry Loss:  tensor(1.1451e+10)\n",
      "Iteration: 5800 \tTotal Loss: tensor(1.1325e+10)\n",
      "IC Loss:  tensor(0.0284) \tBC Loss:  tensor(1.7308e-10) \tNS PDE Loss:  tensor(60.3659) \tMv Bdry Loss:  tensor(1.1319e+10)\n",
      "Iteration: 6000 \tTotal Loss: tensor(1.1279e+10)\n",
      "IC Loss:  tensor(0.0284) \tBC Loss:  tensor(2.1861e-11) \tNS PDE Loss:  tensor(60.3842) \tMv Bdry Loss:  tensor(1.1272e+10)\n",
      "Iteration: 6200 \tTotal Loss: tensor(1.1089e+10)\n",
      "IC Loss:  tensor(0.0285) \tBC Loss:  tensor(2.9897e-09) \tNS PDE Loss:  tensor(60.4010) \tMv Bdry Loss:  tensor(1.1083e+10)\n",
      "Iteration: 6400 \tTotal Loss: tensor(1.1013e+10)\n",
      "IC Loss:  tensor(0.0285) \tBC Loss:  tensor(2.3714e-12) \tNS PDE Loss:  tensor(60.4198) \tMv Bdry Loss:  tensor(1.1007e+10)\n",
      "Iteration: 6600 \tTotal Loss: tensor(1.0928e+10)\n",
      "IC Loss:  tensor(0.0285) \tBC Loss:  tensor(3.5159e-10) \tNS PDE Loss:  tensor(60.4408) \tMv Bdry Loss:  tensor(1.0922e+10)\n",
      "Iteration: 6800 \tTotal Loss: tensor(1.0837e+10)\n",
      "IC Loss:  tensor(0.0286) \tBC Loss:  tensor(2.7060e-10) \tNS PDE Loss:  tensor(60.4632) \tMv Bdry Loss:  tensor(1.0831e+10)\n",
      "Iteration: 7000 \tTotal Loss: tensor(1.0760e+10)\n",
      "IC Loss:  tensor(0.0286) \tBC Loss:  tensor(6.5990e-11) \tNS PDE Loss:  tensor(60.4828) \tMv Bdry Loss:  tensor(1.0754e+10)\n",
      "Iteration: 7200 \tTotal Loss: tensor(1.0665e+10)\n",
      "IC Loss:  tensor(0.0287) \tBC Loss:  tensor(2.4283e-10) \tNS PDE Loss:  tensor(60.5046) \tMv Bdry Loss:  tensor(1.0659e+10)\n",
      "Iteration: 7400 \tTotal Loss: tensor(1.0573e+10)\n",
      "IC Loss:  tensor(0.0287) \tBC Loss:  tensor(1.7611e-11) \tNS PDE Loss:  tensor(60.5209) \tMv Bdry Loss:  tensor(1.0567e+10)\n",
      "Iteration: 7600 \tTotal Loss: tensor(1.0477e+10)\n",
      "IC Loss:  tensor(0.0287) \tBC Loss:  tensor(7.2024e-11) \tNS PDE Loss:  tensor(60.5385) \tMv Bdry Loss:  tensor(1.0471e+10)\n",
      "Iteration: 7800 \tTotal Loss: tensor(1.0424e+10)\n",
      "IC Loss:  tensor(0.0288) \tBC Loss:  tensor(3.3556e-10) \tNS PDE Loss:  tensor(60.5549) \tMv Bdry Loss:  tensor(1.0418e+10)\n",
      "Iteration: 8000 \tTotal Loss: tensor(1.0378e+10)\n",
      "IC Loss:  tensor(0.0288) \tBC Loss:  tensor(8.5808e-10) \tNS PDE Loss:  tensor(60.5713) \tMv Bdry Loss:  tensor(1.0372e+10)\n",
      "Iteration: 8200 \tTotal Loss: tensor(1.0241e+10)\n",
      "IC Loss:  tensor(0.0289) \tBC Loss:  tensor(1.9331e-09) \tNS PDE Loss:  tensor(60.5889) \tMv Bdry Loss:  tensor(1.0235e+10)\n",
      "Iteration: 8400 \tTotal Loss: tensor(1.0171e+10)\n",
      "IC Loss:  tensor(0.0289) \tBC Loss:  tensor(1.5373e-09) \tNS PDE Loss:  tensor(60.6050) \tMv Bdry Loss:  tensor(1.0165e+10)\n",
      "Iteration: 8600 \tTotal Loss: tensor(1.0141e+10)\n",
      "IC Loss:  tensor(0.0289) \tBC Loss:  tensor(1.7882e-10) \tNS PDE Loss:  tensor(60.6210) \tMv Bdry Loss:  tensor(1.0135e+10)\n",
      "Iteration: 8800 \tTotal Loss: tensor(1.0028e+10)\n",
      "IC Loss:  tensor(0.0290) \tBC Loss:  tensor(6.8703e-10) \tNS PDE Loss:  tensor(60.6389) \tMv Bdry Loss:  tensor(1.0022e+10)\n",
      "Iteration: 9000 \tTotal Loss: tensor(1.0045e+10)\n",
      "IC Loss:  tensor(0.0290) \tBC Loss:  tensor(3.2341e-11) \tNS PDE Loss:  tensor(60.6514) \tMv Bdry Loss:  tensor(1.0039e+10)\n",
      "Iteration: 9200 \tTotal Loss: tensor(9.8827e+09)\n",
      "IC Loss:  tensor(0.0291) \tBC Loss:  tensor(4.5050e-11) \tNS PDE Loss:  tensor(60.6627) \tMv Bdry Loss:  tensor(9.8767e+09)\n",
      "Iteration: 9400 \tTotal Loss: tensor(9.8212e+09)\n",
      "IC Loss:  tensor(0.0291) \tBC Loss:  tensor(7.2841e-10) \tNS PDE Loss:  tensor(60.6720) \tMv Bdry Loss:  tensor(9.8151e+09)\n",
      "Iteration: 9600 \tTotal Loss: tensor(9.7480e+09)\n",
      "IC Loss:  tensor(0.0291) \tBC Loss:  tensor(2.5552e-09) \tNS PDE Loss:  tensor(60.6785) \tMv Bdry Loss:  tensor(9.7419e+09)\n",
      "Iteration: 9800 \tTotal Loss: tensor(9.7000e+09)\n",
      "IC Loss:  tensor(0.0292) \tBC Loss:  tensor(1.8760e-11) \tNS PDE Loss:  tensor(60.6851) \tMv Bdry Loss:  tensor(9.6939e+09)\n",
      "Iteration: 10000 \tTotal Loss: tensor(9.6642e+09)\n",
      "IC Loss:  tensor(0.0292) \tBC Loss:  tensor(4.3501e-10) \tNS PDE Loss:  tensor(60.6920) \tMv Bdry Loss:  tensor(9.6581e+09)\n",
      "Iteration: 10200 \tTotal Loss: tensor(9.5663e+09)\n",
      "IC Loss:  tensor(0.0293) \tBC Loss:  tensor(2.5633e-10) \tNS PDE Loss:  tensor(60.6994) \tMv Bdry Loss:  tensor(9.5602e+09)\n",
      "Iteration: 10400 \tTotal Loss: tensor(9.5109e+09)\n",
      "IC Loss:  tensor(0.0293) \tBC Loss:  tensor(1.5475e-09) \tNS PDE Loss:  tensor(60.7032) \tMv Bdry Loss:  tensor(9.5048e+09)\n",
      "Iteration: 10600 \tTotal Loss: tensor(9.4076e+09)\n",
      "IC Loss:  tensor(0.0293) \tBC Loss:  tensor(3.5950e-09) \tNS PDE Loss:  tensor(60.7078) \tMv Bdry Loss:  tensor(9.4015e+09)\n",
      "Iteration: 10800 \tTotal Loss: tensor(9.3837e+09)\n",
      "IC Loss:  tensor(0.0294) \tBC Loss:  tensor(1.4792e-09) \tNS PDE Loss:  tensor(60.7139) \tMv Bdry Loss:  tensor(9.3776e+09)\n",
      "Iteration: 11000 \tTotal Loss: tensor(9.3351e+09)\n",
      "IC Loss:  tensor(0.0294) \tBC Loss:  tensor(3.4528e-09) \tNS PDE Loss:  tensor(60.7190) \tMv Bdry Loss:  tensor(9.3290e+09)\n",
      "Iteration: 11200 \tTotal Loss: tensor(9.2658e+09)\n",
      "IC Loss:  tensor(0.0295) \tBC Loss:  tensor(1.0970e-10) \tNS PDE Loss:  tensor(60.7249) \tMv Bdry Loss:  tensor(9.2597e+09)\n",
      "Iteration: 11400 \tTotal Loss: tensor(9.1993e+09)\n",
      "IC Loss:  tensor(0.0295) \tBC Loss:  tensor(1.0264e-10) \tNS PDE Loss:  tensor(60.7273) \tMv Bdry Loss:  tensor(9.1933e+09)\n",
      "Iteration: 11600 \tTotal Loss: tensor(9.1553e+09)\n",
      "IC Loss:  tensor(0.0295) \tBC Loss:  tensor(3.9717e-12) \tNS PDE Loss:  tensor(60.7310) \tMv Bdry Loss:  tensor(9.1493e+09)\n",
      "Iteration: 11800 \tTotal Loss: tensor(9.0437e+09)\n",
      "IC Loss:  tensor(0.0296) \tBC Loss:  tensor(4.5430e-10) \tNS PDE Loss:  tensor(60.7346) \tMv Bdry Loss:  tensor(9.0376e+09)\n",
      "Iteration: 12000 \tTotal Loss: tensor(9.1433e+09)\n",
      "IC Loss:  tensor(0.0296) \tBC Loss:  tensor(1.3930e-10) \tNS PDE Loss:  tensor(60.7391) \tMv Bdry Loss:  tensor(9.1372e+09)\n",
      "Iteration: 12200 \tTotal Loss: tensor(8.9802e+09)\n",
      "IC Loss:  tensor(0.0296) \tBC Loss:  tensor(1.0348e-11) \tNS PDE Loss:  tensor(60.7415) \tMv Bdry Loss:  tensor(8.9741e+09)\n",
      "Iteration: 12400 \tTotal Loss: tensor(8.9073e+09)\n",
      "IC Loss:  tensor(0.0296) \tBC Loss:  tensor(8.8079e-12) \tNS PDE Loss:  tensor(60.7444) \tMv Bdry Loss:  tensor(8.9012e+09)\n",
      "Iteration: 12600 \tTotal Loss: tensor(8.8783e+09)\n",
      "IC Loss:  tensor(0.0297) \tBC Loss:  tensor(6.6323e-10) \tNS PDE Loss:  tensor(60.7463) \tMv Bdry Loss:  tensor(8.8723e+09)\n",
      "Iteration: 12800 \tTotal Loss: tensor(8.7814e+09)\n",
      "IC Loss:  tensor(0.0297) \tBC Loss:  tensor(1.4694e-10) \tNS PDE Loss:  tensor(60.7462) \tMv Bdry Loss:  tensor(8.7753e+09)\n",
      "Iteration: 13000 \tTotal Loss: tensor(8.7794e+09)\n",
      "IC Loss:  tensor(0.0297) \tBC Loss:  tensor(3.8750e-11) \tNS PDE Loss:  tensor(60.7469) \tMv Bdry Loss:  tensor(8.7733e+09)\n",
      "Iteration: 13200 \tTotal Loss: tensor(8.7393e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(8.8484e-11) \tNS PDE Loss:  tensor(60.7483) \tMv Bdry Loss:  tensor(8.7332e+09)\n",
      "Iteration: 13400 \tTotal Loss: tensor(8.6480e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(2.8761e-12) \tNS PDE Loss:  tensor(60.7481) \tMv Bdry Loss:  tensor(8.6419e+09)\n",
      "Iteration: 13600 \tTotal Loss: tensor(8.6346e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(1.4387e-12) \tNS PDE Loss:  tensor(60.7486) \tMv Bdry Loss:  tensor(8.6285e+09)\n",
      "Iteration: 13800 \tTotal Loss: tensor(8.5673e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(1.4668e-10) \tNS PDE Loss:  tensor(60.7484) \tMv Bdry Loss:  tensor(8.5612e+09)\n",
      "Iteration: 14000 \tTotal Loss: tensor(8.5632e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(4.8697e-11) \tNS PDE Loss:  tensor(60.7458) \tMv Bdry Loss:  tensor(8.5571e+09)\n",
      "Iteration: 14200 \tTotal Loss: tensor(8.4095e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(2.0810e-11) \tNS PDE Loss:  tensor(60.7357) \tMv Bdry Loss:  tensor(8.4034e+09)\n",
      "Iteration: 14400 \tTotal Loss: tensor(8.2798e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(3.5464e-10) \tNS PDE Loss:  tensor(60.7251) \tMv Bdry Loss:  tensor(8.2737e+09)\n",
      "Iteration: 14600 \tTotal Loss: tensor(8.1280e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(1.4600e-10) \tNS PDE Loss:  tensor(60.7331) \tMv Bdry Loss:  tensor(8.1219e+09)\n",
      "Iteration: 14800 \tTotal Loss: tensor(7.9988e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.3425e-09) \tNS PDE Loss:  tensor(60.7274) \tMv Bdry Loss:  tensor(7.9927e+09)\n",
      "Iteration: 15000 \tTotal Loss: tensor(7.9360e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.2147e-10) \tNS PDE Loss:  tensor(60.7201) \tMv Bdry Loss:  tensor(7.9299e+09)\n",
      "Iteration: 15200 \tTotal Loss: tensor(7.8899e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(1.4165e-12) \tNS PDE Loss:  tensor(60.7134) \tMv Bdry Loss:  tensor(7.8839e+09)\n",
      "Iteration: 15400 \tTotal Loss: tensor(7.8103e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(3.4856e-11) \tNS PDE Loss:  tensor(60.7093) \tMv Bdry Loss:  tensor(7.8042e+09)\n",
      "Iteration: 15600 \tTotal Loss: tensor(7.7447e+09)\n",
      "IC Loss:  tensor(0.0301) \tBC Loss:  tensor(7.9473e-11) \tNS PDE Loss:  tensor(60.7106) \tMv Bdry Loss:  tensor(7.7386e+09)\n",
      "Iteration: 15800 \tTotal Loss: tensor(7.6782e+09)\n",
      "IC Loss:  tensor(0.0301) \tBC Loss:  tensor(6.7441e-10) \tNS PDE Loss:  tensor(60.7152) \tMv Bdry Loss:  tensor(7.6721e+09)\n",
      "Iteration: 16000 \tTotal Loss: tensor(7.6460e+09)\n",
      "IC Loss:  tensor(0.0301) \tBC Loss:  tensor(6.4279e-10) \tNS PDE Loss:  tensor(60.7191) \tMv Bdry Loss:  tensor(7.6399e+09)\n",
      "Iteration: 16200 \tTotal Loss: tensor(7.2453e+09)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(1.3245e-12) \tNS PDE Loss:  tensor(60.7659) \tMv Bdry Loss:  tensor(7.2392e+09)\n",
      "Iteration: 16400 \tTotal Loss: tensor(7.1082e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(8.7392e-10) \tNS PDE Loss:  tensor(60.6933) \tMv Bdry Loss:  tensor(7.1021e+09)\n",
      "Iteration: 16600 \tTotal Loss: tensor(6.3300e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.3018e-11) \tNS PDE Loss:  tensor(60.6748) \tMv Bdry Loss:  tensor(6.3239e+09)\n",
      "Iteration: 16800 \tTotal Loss: tensor(6.3133e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(2.7090e-11) \tNS PDE Loss:  tensor(60.6568) \tMv Bdry Loss:  tensor(6.3073e+09)\n",
      "Iteration: 17000 \tTotal Loss: tensor(6.1487e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(3.7776e-10) \tNS PDE Loss:  tensor(60.6452) \tMv Bdry Loss:  tensor(6.1426e+09)\n",
      "Iteration: 17200 \tTotal Loss: tensor(6.0778e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(1.3967e-11) \tNS PDE Loss:  tensor(60.6091) \tMv Bdry Loss:  tensor(6.0717e+09)\n",
      "Iteration: 17400 \tTotal Loss: tensor(5.9432e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(2.6048e-10) \tNS PDE Loss:  tensor(60.5615) \tMv Bdry Loss:  tensor(5.9371e+09)\n",
      "Iteration: 17600 \tTotal Loss: tensor(5.7350e+09)\n",
      "IC Loss:  tensor(0.0297) \tBC Loss:  tensor(1.6428e-10) \tNS PDE Loss:  tensor(60.5555) \tMv Bdry Loss:  tensor(5.7290e+09)\n",
      "Iteration: 17800 \tTotal Loss: tensor(5.7187e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(2.4788e-12) \tNS PDE Loss:  tensor(60.5490) \tMv Bdry Loss:  tensor(5.7126e+09)\n",
      "Iteration: 18000 \tTotal Loss: tensor(5.7062e+09)\n",
      "IC Loss:  tensor(0.0298) \tBC Loss:  tensor(9.3882e-10) \tNS PDE Loss:  tensor(60.5503) \tMv Bdry Loss:  tensor(5.7002e+09)\n",
      "Iteration: 18200 \tTotal Loss: tensor(5.5703e+09)\n",
      "IC Loss:  tensor(0.0299) \tBC Loss:  tensor(5.5169e-10) \tNS PDE Loss:  tensor(60.5403) \tMv Bdry Loss:  tensor(5.5642e+09)\n",
      "Iteration: 18400 \tTotal Loss: tensor(5.4353e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(1.9936e-11) \tNS PDE Loss:  tensor(60.5386) \tMv Bdry Loss:  tensor(5.4292e+09)\n",
      "Iteration: 18600 \tTotal Loss: tensor(5.6080e+09)\n",
      "IC Loss:  tensor(0.0300) \tBC Loss:  tensor(1.2001e-09) \tNS PDE Loss:  tensor(60.5411) \tMv Bdry Loss:  tensor(5.6019e+09)\n",
      "Iteration: 18800 \tTotal Loss: tensor(5.3918e+09)\n",
      "IC Loss:  tensor(0.0301) \tBC Loss:  tensor(2.3372e-10) \tNS PDE Loss:  tensor(60.5475) \tMv Bdry Loss:  tensor(5.3857e+09)\n",
      "Iteration: 19000 \tTotal Loss: tensor(5.4558e+09)\n",
      "IC Loss:  tensor(0.0301) \tBC Loss:  tensor(4.8703e-10) \tNS PDE Loss:  tensor(60.5417) \tMv Bdry Loss:  tensor(5.4497e+09)\n",
      "Iteration: 19200 \tTotal Loss: tensor(5.2876e+09)\n",
      "IC Loss:  tensor(0.0302) \tBC Loss:  tensor(6.0214e-11) \tNS PDE Loss:  tensor(60.5398) \tMv Bdry Loss:  tensor(5.2815e+09)\n",
      "Iteration: 19400 \tTotal Loss: tensor(5.2701e+09)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(9.5158e-13) \tNS PDE Loss:  tensor(60.4953) \tMv Bdry Loss:  tensor(5.2641e+09)\n",
      "Iteration: 19600 \tTotal Loss: tensor(5.1427e+09)\n",
      "IC Loss:  tensor(0.0304) \tBC Loss:  tensor(4.4441e-10) \tNS PDE Loss:  tensor(60.4751) \tMv Bdry Loss:  tensor(5.1366e+09)\n",
      "Iteration: 19800 \tTotal Loss: tensor(5.1999e+09)\n",
      "IC Loss:  tensor(0.0303) \tBC Loss:  tensor(1.1056e-09) \tNS PDE Loss:  tensor(60.4801) \tMv Bdry Loss:  tensor(5.1938e+09)\n",
      "Iteration: 20000 \tTotal Loss: tensor(5.5039e+09)\n",
      "IC Loss:  tensor(0.0304) \tBC Loss:  tensor(8.0143e-11) \tNS PDE Loss:  tensor(60.4576) \tMv Bdry Loss:  tensor(5.4978e+09)\n",
      "Total Time:\t 7318.642845869064 \n",
      "Pass 1 Time:\t 7318.640991926193 \n",
      "Pass 2 Time:\t -1719258115.618411 \n",
      "Pass 3 Time:\t -1719258115.618411 \n",
      "Pass 4 Time:\t -1719258115.618411\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "#Call model of layers and its forward step\n",
    "from Forward_with_Layer_Setting import Net\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Call training functions of Loss functions\n",
    "from NSpde_loss import lossNSpde\n",
    "from MvBdry_Coefficient_and_Loss import lossMvBdry \n",
    "from BoundaryLoss import lossBdry\n",
    "from InitialConditionLoss import lossIC, lossIC_with\n",
    "\n",
    "\n",
    "\n",
    "def create_network(IC_Only_Train):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"NNlayers_Bubble_0_6_24.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "    #Set final times for running training\n",
    "    time_slices = np.array([.01,.1]) #, .25, .5, 1\n",
    "    \n",
    "    #Load Training Points\n",
    "    x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry = twoDimTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100))\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    #Start Training only on IC\n",
    "    if IC_Only_Train == True:\n",
    "        print('Training Only on the Initial Condition')\n",
    "        Create_IC_Parameters(x_domain, y_domain, t_zero, 30000, 10**-3, 'IC_Only.pt', record_loss = 100, print_loss = 1000)\n",
    "        IC_Done = time.time()\n",
    "        print('IC Time:\\t', IC_Done-start)\n",
    "        '''\n",
    "        print('Training Only on the NSpde Condition')\n",
    "        NSpde_Only_training(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, \n",
    "                      y_l_Bdry, y_u_Bdry, time_slices, 20000, 10**-3, record_loss = 100, print_loss = 1000)\n",
    "        torch.save(net.state_dict(), f\"NSpde_afterIC_.pt\")\n",
    "        NSpde_Done = time.time()\n",
    "        print('MvBdry Time:\\t', NSpde_Done-start)\n",
    "        '''\n",
    "        return 0\n",
    "        \n",
    "    time_vec = [0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    #attempt to load IC if it exists\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(\"IC_Only.pt\"))\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "    #attempt to load MvBdry if it exists\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(\"NSpde_Only.pt\"))\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "    global epsilon #used to track loss\n",
    "    epsilon = []\n",
    "    \n",
    "    print('Training PDE')\n",
    "    \n",
    "    for i in range(1): #4\n",
    "        #Set loop to optimize in progressively smaller learning rates\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print('Executing Pass 1')\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-4    \n",
    "        elif i == 1:\n",
    "            print('Executing Pass 2')\n",
    "            #time_slices = time_slices[-1]\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-5\n",
    "        elif i == 2:\n",
    "            print('Executing Pass 3')\n",
    "            iterations = 20000\n",
    "            learning_rate = 5*10**-6\n",
    "        elif i ==3:\n",
    "            print('Executing Pass 4')\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-6\n",
    "        \n",
    "        training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, \n",
    "                      y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient = 1, record_loss = 100, print_loss = 200)\n",
    "        torch.save(net.state_dict(), f\"NNlayers_Bubble_{i}.pt\")\n",
    "        np.savetxt('epsilon.txt', epsilon)\n",
    "        time_vec[i] = time.time()\n",
    "\n",
    "    np.savetxt('epsilon.txt', epsilon)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Total Time:\\t\", end-start, '\\nPass 1 Time:\\t', time_vec[0]-start, '\\nPass 2 Time:\\t', time_vec[1]-start, '\\nPass 3 Time:\\t', time_vec[2]-start, '\\nPass 4 Time:\\t', time_vec[3]-start)\n",
    "\n",
    "\n",
    "def twoDimTrainPts(net, Domain_collocation, Bdry_collocation):\n",
    "    #Set of all the recorded xy variables as base data for chasing during training\n",
    "    \n",
    "    # Domain boundary in the range [0, 1]x[0, 2] and time in [0, 1].\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    y_l = net.x2_l\n",
    "    y_u = net.x2_u\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = 0\n",
    "\n",
    "    #Pick IC/Mv Bdry/NSpde Condition Training Random Points in Numpy\n",
    "    x_domain = np.random.uniform(low= x_l, high=x_u, size=(Domain_collocation, 1)) \n",
    "    y_domain = np.random.uniform(low= y_l, high=y_u, size=(Domain_collocation, 1)) \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "    y_domain = Variable(torch.from_numpy(y_domain).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(x_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    x_Bdry = np.random.uniform(low=x_l, high=x_u, size=(Bdry_collocation,1))\n",
    "    y_Bdry = np.random.uniform(low=y_l, high=y_u, size=(Bdry_collocation,1))       \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_Bdry= Variable(torch.from_numpy(x_Bdry).float(), requires_grad=True).to(device)\n",
    "    y_Bdry = Variable(torch.from_numpy(y_Bdry).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    ##Pick pts to make tensor for No-Slip Boundary Condition\n",
    "    x_l_Bdry = Variable(x_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    x_u_Bdry = Variable(x_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_l_Bdry = Variable(y_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_u_Bdry = Variable(y_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    \n",
    "            \n",
    "    return x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry\n",
    "    \n",
    "def tsliceTrainPts(net, Domain_collocation, Bdry_collocation, final_time):\n",
    "    #Set of all the recorded t variable as base data for chasing during training\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = net.t_l\n",
    "\n",
    "    #Pick IC/Mv Bdry/NSpde Condition Training Random Points in Numpy\n",
    "    t_domain = np.random.uniform(low=t_l, high=final_time, size=(Domain_collocation, 1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(t_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    t_Bdry = np.random.uniform(low=t_l, high=final_time, size=(Bdry_collocation,1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_Bdry = Variable(torch.from_numpy(t_Bdry).float(), requires_grad=True).to(device)\n",
    "        \n",
    "    return t_domain, t_Bdry\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def Create_IC_Parameters(x_domain, y_domain, t_zero, iterations, learning_rate, filename, record_loss, print_loss):\n",
    "    ICnet = Net().to(device)\n",
    "    \n",
    "    IC_Only_training(ICnet, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss)\n",
    "    \n",
    "    torch.save(ICnet.state_dict(), filename)\n",
    "    \n",
    "\n",
    "def IC_Only_training(net, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Domain boundary in the range [0, 1]x[0, 2] and time in [0, 1].\n",
    "    #x_l = net.x1_l\n",
    "    #x_u = net.x1_u\n",
    "    #y_l = net.x2_l\n",
    "    #y_u = net.x2_u\n",
    "\n",
    "    ##Define Colloacation Points with Initial Condition\n",
    "    #IC_collocation = collocation\n",
    "    \n",
    "    #define in numpy\n",
    "    #x_IC = np.random.uniform(low=x_l, high=x_u, size=(IC_collocation,1))\n",
    "    #y_IC = np.random.uniform(low=y_l, high=y_u, size=(IC_collocation,1))\n",
    "    \n",
    "    #move to pytorch tensors\n",
    "    #input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "    #input_y_IC = Variable(torch.from_numpy(y_IC).float(), requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #training loop\n",
    "    epsilon_IC = [] #placeholder to track decreasing loss\n",
    "    for epoch in range(1, iterations+1):\n",
    "        \n",
    "    \n",
    "        # Resetting gradients to zero\n",
    "        net.optimizer.zero_grad()\n",
    "           \n",
    "        #Loss based on Initial Condition\n",
    "        loss = lossIC(net, x_domain, y_domain, t_zero)\n",
    "           \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Norm Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=2, error_if_nonfinite=True)\n",
    "\n",
    "        #Gradient Value Clipping\n",
    "        #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "        \n",
    "        net.optimizer.step()\n",
    "           \n",
    "        #Print Loss every 1000 Epochs\n",
    "        with torch.autograd.no_grad():\n",
    "            \n",
    "            if epoch%record_loss == 0:\n",
    "                epsilon_IC = np.append(epsilon_IC, loss.cpu().detach().numpy())\n",
    "            if epoch%print_loss == 0:\n",
    "                print(\"Iteration:\", epoch, \"Initial Condition Loss:\", loss.data)\n",
    "    \n",
    "    np.savetxt('epsilon_IC.txt', epsilon_IC)\n",
    "\n",
    "def NSpde_Only_training(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, record_loss, print_loss):\n",
    "    global epsilon\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        #indicator = False\n",
    "        reset_regularization = 1000\n",
    "        \n",
    "        #Iterate over these points\n",
    "        \n",
    "        t_domain, t_Bdry = tsliceTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100), final_time = final_time)    \n",
    "        for epoch in range(1, iterations+1):\n",
    "            # Loss calculation based on partial differential equation (PDE) \n",
    "            \n",
    "            if epoch%reset_regularization != 0: #To detect error on forward/Backward, add hashtag on this whole line, and\n",
    "            #with torch.autograd.detect_anomaly(): #use this line alternatively by deleting hashtag.\n",
    "                \n",
    "                ###Training steps\n",
    "                # Resetting gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "            \n",
    "                '''\n",
    "                #Loss based on Initial Condition\n",
    "                mse_IC = lossIC(net, x_domain, y_domain, t_zero)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC = lossBdry(net, x_Bdry, y_Bdry, t_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "                '''\n",
    "                #Loss based on PDE\n",
    "                mse_NS = lossNSpde(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "                '''\n",
    "            \n",
    "                #Loss based on Moving Boundary\n",
    "                mse_MvBdry = lossMvBdry(net, x_domain, y_domain, t_domain)\n",
    "                \n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                \n",
    "                if indicator == False:\n",
    "                    indicator = True\n",
    "                    IC_regular = mse_IC.detach()\n",
    "                    BC_regular = mse_BC.detach()\n",
    "                    pde_regular = mse_NS.detach()\n",
    "                    MvBdry_regular = mse_MvBdry.detach()\n",
    "                \n",
    "                raw_loss = mse_MvBdry\n",
    "                \n",
    "                mse_IC = mse_IC #/IC_regular\n",
    "                mse_BC = mse_BC #/BC_regular\n",
    "                mse_NS = mse_NS #/pde_regular\n",
    "                mse_MvBdry = mse_MvBdry #/MvBdry_regular\n",
    "                '''\n",
    "                #Combine all Loss functions\n",
    "                loss = 10**7 * mse_NS \n",
    "                # Gradient Norm Clipping\n",
    "                #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                loss.backward()\n",
    "            \n",
    "            #Gradient Value Clipping\n",
    "            #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch%print_loss == 0:\n",
    "                    print(\"Iteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"\\tNspde Loss: \", mse_NS.data)\n",
    "                    \n",
    "\n",
    "def training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient, record_loss, print_loss):\n",
    "    global epsilon\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        indicator = False\n",
    "        reset_regularization = 1000\n",
    "        \n",
    "        #Iterate over these points\n",
    "        \n",
    "        t_domain, t_Bdry = tsliceTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100), final_time = final_time)    \n",
    "        for epoch in range(1, iterations+1):\n",
    "            # Loss calculation based on partial differential equation (PDE) \n",
    "            \n",
    "            if epoch%reset_regularization == 0:\n",
    "                indicator = False\n",
    "    \n",
    "            if epoch%reset_regularization != 0: #To detect error on forward/Backward, add hashtag on this whole line, and\n",
    "            #with torch.autograd.detect_anomaly(): #use this line alternatively by deleting hashtag.\n",
    "                \n",
    "                ###Training steps\n",
    "                # Resetting gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "            \n",
    "                #Loss based on Initial Condition\n",
    "                mse_IC = lossIC(net, x_domain, y_domain, t_zero)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC = lossBdry(net, x_Bdry, y_Bdry, t_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on PDE\n",
    "                mse_NS = lossNSpde(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            \n",
    "                #Loss based on Moving Boundary\n",
    "                mse_MvBdry = lossMvBdry(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            \n",
    "                if indicator == False:\n",
    "                    indicator = True\n",
    "                    IC_regular = mse_IC.detach()\n",
    "                    BC_regular = mse_BC.detach()\n",
    "                    pde_regular = mse_NS.detach()\n",
    "                    MvBdry_regular = mse_MvBdry.detach()\n",
    "                \n",
    "                raw_loss = IC_coefficient * mse_IC + mse_BC + mse_NS + mse_MvBdry\n",
    "            \n",
    "                mse_IC = mse_IC #/IC_regular\n",
    "                mse_BC = mse_BC #/BC_regular\n",
    "                mse_NS = mse_NS #/pde_regular\n",
    "                mse_MvBdry = mse_MvBdry #/MvBdry_regular\n",
    "            \n",
    "                #Combine all Loss functions\n",
    "                loss = mse_BC +mse_MvBdry + 10**5 *mse_IC  + 10**5 * mse_NS #IC_coefficient *\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                loss.backward()\n",
    "            # Gradient Norm Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            #Gradient Value Clipping\n",
    "            #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch%record_loss == 0:\n",
    "                    epsilon = np.append(epsilon, raw_loss.cpu().detach().numpy())\n",
    "                if epoch%print_loss == 0:\n",
    "                    print(\"Iteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"IC Loss: \", mse_IC.data, \"\\tBC Loss: \", mse_BC.data, \"\\tNS PDE Loss: \", mse_NS.data, \"\\tMv Bdry Loss: \", mse_MvBdry.data)\n",
    "\n",
    "            \n",
    "                \n",
    "create_network(True)\n",
    "create_network(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13182c8-069d-49cc-b89f-0a6289d603d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
