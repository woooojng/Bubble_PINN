{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8322d-5a4c-4bc7-9219-82163d85a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Only on the Initial Condition\n",
      "Iteration: 1000 Initial Condition Loss: tensor(0.1442)\n",
      "Iteration: 2000 Initial Condition Loss: tensor(0.0330)\n",
      "Iteration: 3000 Initial Condition Loss: tensor(0.0181)\n",
      "Iteration: 4000 Initial Condition Loss: tensor(0.0106)\n",
      "Iteration: 5000 Initial Condition Loss: tensor(0.0071)\n",
      "Iteration: 6000 Initial Condition Loss: tensor(0.0050)\n",
      "Iteration: 7000 Initial Condition Loss: tensor(0.0036)\n",
      "Iteration: 8000 Initial Condition Loss: tensor(0.0026)\n",
      "Iteration: 9000 Initial Condition Loss: tensor(0.0023)\n",
      "Iteration: 10000 Initial Condition Loss: tensor(0.0018)\n",
      "Iteration: 11000 Initial Condition Loss: tensor(0.0014)\n",
      "Iteration: 12000 Initial Condition Loss: tensor(0.0011)\n",
      "Iteration: 13000 Initial Condition Loss: tensor(0.0008)\n",
      "Iteration: 14000 Initial Condition Loss: tensor(0.0006)\n",
      "Iteration: 15000 Initial Condition Loss: tensor(0.0004)\n",
      "Iteration: 16000 Initial Condition Loss: tensor(0.0010)\n",
      "Iteration: 17000 Initial Condition Loss: tensor(0.0003)\n",
      "Iteration: 18000 Initial Condition Loss: tensor(0.0003)\n",
      "Iteration: 19000 Initial Condition Loss: tensor(0.0002)\n",
      "Iteration: 20000 Initial Condition Loss: tensor(0.0002)\n",
      "Iteration: 21000 Initial Condition Loss: tensor(0.0002)\n",
      "Iteration: 22000 Initial Condition Loss: tensor(0.0001)\n",
      "Iteration: 23000 Initial Condition Loss: tensor(0.0002)\n",
      "Iteration: 24000 Initial Condition Loss: tensor(0.0001)\n",
      "Iteration: 25000 Initial Condition Loss: tensor(0.0001)\n",
      "Iteration: 26000 Initial Condition Loss: tensor(0.0001)\n",
      "Iteration: 27000 Initial Condition Loss: tensor(9.0149e-05)\n",
      "Iteration: 28000 Initial Condition Loss: tensor(7.5147e-05)\n",
      "Iteration: 29000 Initial Condition Loss: tensor(5.8029e-05)\n",
      "Iteration: 30000 Initial Condition Loss: tensor(4.1409e-05)\n",
      "Iteration: 31000 Initial Condition Loss: tensor(5.7615e-05)\n",
      "Iteration: 32000 Initial Condition Loss: tensor(4.3143e-05)\n",
      "Iteration: 33000 Initial Condition Loss: tensor(3.8612e-05)\n",
      "Iteration: 34000 Initial Condition Loss: tensor(3.5494e-05)\n",
      "Iteration: 35000 Initial Condition Loss: tensor(3.1781e-05)\n",
      "Iteration: 36000 Initial Condition Loss: tensor(2.7131e-05)\n",
      "Iteration: 37000 Initial Condition Loss: tensor(2.1596e-05)\n",
      "Iteration: 38000 Initial Condition Loss: tensor(1.5683e-05)\n",
      "Iteration: 39000 Initial Condition Loss: tensor(1.0978e-05)\n",
      "Iteration: 40000 Initial Condition Loss: tensor(3.1538e-05)\n",
      "Iteration: 41000 Initial Condition Loss: tensor(2.1425e-05)\n",
      "Iteration: 42000 Initial Condition Loss: tensor(1.7270e-05)\n",
      "Iteration: 43000 Initial Condition Loss: tensor(1.4976e-05)\n",
      "Iteration: 44000 Initial Condition Loss: tensor(1.3218e-05)\n",
      "Iteration: 45000 Initial Condition Loss: tensor(1.1820e-05)\n",
      "Iteration: 46000 Initial Condition Loss: tensor(1.0491e-05)\n",
      "Iteration: 47000 Initial Condition Loss: tensor(8.9294e-06)\n",
      "Iteration: 48000 Initial Condition Loss: tensor(7.1929e-06)\n",
      "Iteration: 49000 Initial Condition Loss: tensor(5.3927e-06)\n",
      "Iteration: 50000 Initial Condition Loss: tensor(3.7994e-06)\n",
      "IC Time:\t 437.8927879333496\n",
      "Training Only on the NSpde Condition\n",
      "Current Final Time: 0.01 Current Learning Rate:  0.001\n",
      "Iteration: 1000 \tTotal Loss: tensor(3740.0942)\n",
      "\tNspde Loss:  tensor(0.0004)\n",
      "Iteration: 2000 \tTotal Loss: tensor(926.5115)\n",
      "\tNspde Loss:  tensor(9.2651e-05)\n",
      "Iteration: 3000 \tTotal Loss: tensor(326.1009)\n",
      "\tNspde Loss:  tensor(3.2610e-05)\n",
      "Iteration: 4000 \tTotal Loss: tensor(146.5340)\n",
      "\tNspde Loss:  tensor(1.4653e-05)\n",
      "Iteration: 5000 \tTotal Loss: tensor(84.6880)\n",
      "\tNspde Loss:  tensor(8.4688e-06)\n",
      "Current Final Time: 0.1 Current Learning Rate:  0.001\n",
      "Iteration: 1000 \tTotal Loss: tensor(54.4744)\n",
      "\tNspde Loss:  tensor(5.4474e-06)\n",
      "Iteration: 2000 \tTotal Loss: tensor(34.6120)\n",
      "\tNspde Loss:  tensor(3.4612e-06)\n",
      "Iteration: 3000 \tTotal Loss: tensor(21.7702)\n",
      "\tNspde Loss:  tensor(2.1770e-06)\n",
      "Iteration: 4000 \tTotal Loss: tensor(23.4788)\n",
      "\tNspde Loss:  tensor(2.3479e-06)\n",
      "Iteration: 5000 \tTotal Loss: tensor(21.2125)\n",
      "\tNspde Loss:  tensor(2.1212e-06)\n",
      "MvBdry Time:\t 1233.4364969730377\n",
      "Training PDE\n",
      "Executing Pass 1\n",
      "Current Final Time: 0.01 Current Learning Rate:  0.0001\n",
      "Iteration: 200 \tTotal Loss: tensor(4.9827e+10)\n",
      "IC Loss:  tensor(11990.2158) \tBC Loss:  tensor(9.5472e-10) \tNS PDE Loss:  tensor(52.2697) \tMv Bdry Loss:  tensor(4.8623e+10)\n",
      "Iteration: 400 \tTotal Loss: tensor(2.8851e+10)\n",
      "IC Loss:  tensor(12001.0938) \tBC Loss:  tensor(1.0615e-09) \tNS PDE Loss:  tensor(52.0903) \tMv Bdry Loss:  tensor(2.7646e+10)\n",
      "Iteration: 600 \tTotal Loss: tensor(1.4430e+10)\n",
      "IC Loss:  tensor(12010.1660) \tBC Loss:  tensor(3.2092e-11) \tNS PDE Loss:  tensor(51.9665) \tMv Bdry Loss:  tensor(1.3224e+10)\n",
      "Iteration: 800 \tTotal Loss: tensor(1.1274e+10)\n",
      "IC Loss:  tensor(12007.2500) \tBC Loss:  tensor(2.6268e-10) \tNS PDE Loss:  tensor(51.9664) \tMv Bdry Loss:  tensor(1.0068e+10)\n",
      "Iteration: 1000 \tTotal Loss: tensor(1.0018e+11)\n",
      "IC Loss:  tensor(12008.2529) \tBC Loss:  tensor(4.8827e-10) \tNS PDE Loss:  tensor(52.0131) \tMv Bdry Loss:  tensor(9.8979e+10)\n",
      "Iteration: 1200 \tTotal Loss: tensor(4.4010e+10)\n",
      "IC Loss:  tensor(12008.1104) \tBC Loss:  tensor(1.3802e-10) \tNS PDE Loss:  tensor(51.7312) \tMv Bdry Loss:  tensor(4.2804e+10)\n",
      "Iteration: 1400 \tTotal Loss: tensor(9.9714e+09)\n",
      "IC Loss:  tensor(12018.6240) \tBC Loss:  tensor(9.8112e-11) \tNS PDE Loss:  tensor(51.5206) \tMv Bdry Loss:  tensor(8.7644e+09)\n",
      "Iteration: 1600 \tTotal Loss: tensor(8.3974e+09)\n",
      "IC Loss:  tensor(12009.8896) \tBC Loss:  tensor(7.9423e-11) \tNS PDE Loss:  tensor(51.8452) \tMv Bdry Loss:  tensor(7.1912e+09)\n",
      "Iteration: 1800 \tTotal Loss: tensor(1.2338e+10)\n",
      "IC Loss:  tensor(12012.0811) \tBC Loss:  tensor(7.3527e-11) \tNS PDE Loss:  tensor(51.9150) \tMv Bdry Loss:  tensor(1.1132e+10)\n",
      "Iteration: 2000 \tTotal Loss: tensor(1.5796e+10)\n",
      "IC Loss:  tensor(12018.3926) \tBC Loss:  tensor(7.2628e-11) \tNS PDE Loss:  tensor(51.8388) \tMv Bdry Loss:  tensor(1.4589e+10)\n",
      "Iteration: 2200 \tTotal Loss: tensor(1.1532e+10)\n",
      "IC Loss:  tensor(12013.3457) \tBC Loss:  tensor(7.3196e-11) \tNS PDE Loss:  tensor(51.8064) \tMv Bdry Loss:  tensor(1.0326e+10)\n",
      "Iteration: 2400 \tTotal Loss: tensor(6.8717e+09)\n",
      "IC Loss:  tensor(12012.6768) \tBC Loss:  tensor(7.3507e-11) \tNS PDE Loss:  tensor(51.9562) \tMv Bdry Loss:  tensor(5.6652e+09)\n",
      "Iteration: 2600 \tTotal Loss: tensor(7.1210e+09)\n",
      "IC Loss:  tensor(12009.7803) \tBC Loss:  tensor(7.2812e-11) \tNS PDE Loss:  tensor(52.0282) \tMv Bdry Loss:  tensor(5.9148e+09)\n",
      "Iteration: 2800 \tTotal Loss: tensor(1.7687e+10)\n",
      "IC Loss:  tensor(12003.4590) \tBC Loss:  tensor(7.4580e-11) \tNS PDE Loss:  tensor(52.1703) \tMv Bdry Loss:  tensor(1.6481e+10)\n",
      "Iteration: 3000 \tTotal Loss: tensor(8.1798e+09)\n",
      "IC Loss:  tensor(12013.1221) \tBC Loss:  tensor(7.3729e-11) \tNS PDE Loss:  tensor(51.9958) \tMv Bdry Loss:  tensor(6.9733e+09)\n",
      "Iteration: 3200 \tTotal Loss: tensor(7.8312e+09)\n",
      "IC Loss:  tensor(12023.6885) \tBC Loss:  tensor(7.4486e-11) \tNS PDE Loss:  tensor(51.8653) \tMv Bdry Loss:  tensor(6.6236e+09)\n",
      "Iteration: 3400 \tTotal Loss: tensor(8.3612e+10)\n",
      "IC Loss:  tensor(11958.3945) \tBC Loss:  tensor(7.1401e-11) \tNS PDE Loss:  tensor(52.5621) \tMv Bdry Loss:  tensor(8.2410e+10)\n",
      "Iteration: 3600 \tTotal Loss: tensor(7.4306e+09)\n",
      "IC Loss:  tensor(11997.3857) \tBC Loss:  tensor(6.7955e-11) \tNS PDE Loss:  tensor(52.2180) \tMv Bdry Loss:  tensor(6.2256e+09)\n",
      "Iteration: 3800 \tTotal Loss: tensor(6.8586e+09)\n",
      "IC Loss:  tensor(11996.2861) \tBC Loss:  tensor(6.3603e-11) \tNS PDE Loss:  tensor(52.2007) \tMv Bdry Loss:  tensor(5.6537e+09)\n",
      "Iteration: 4000 \tTotal Loss: tensor(6.7706e+09)\n",
      "IC Loss:  tensor(12007.4619) \tBC Loss:  tensor(6.0168e-11) \tNS PDE Loss:  tensor(52.0242) \tMv Bdry Loss:  tensor(5.5647e+09)\n",
      "Iteration: 4200 \tTotal Loss: tensor(2.1943e+11)\n",
      "IC Loss:  tensor(12032.4131) \tBC Loss:  tensor(1.7365e-11) \tNS PDE Loss:  tensor(51.7117) \tMv Bdry Loss:  tensor(2.1822e+11)\n",
      "Iteration: 4400 \tTotal Loss: tensor(4.4242e+10)\n",
      "IC Loss:  tensor(12008.2764) \tBC Loss:  tensor(1.5692e-11) \tNS PDE Loss:  tensor(51.8495) \tMv Bdry Loss:  tensor(4.3036e+10)\n",
      "Iteration: 4600 \tTotal Loss: tensor(6.9984e+09)\n",
      "IC Loss:  tensor(12005.8057) \tBC Loss:  tensor(1.0916e-11) \tNS PDE Loss:  tensor(52.1385) \tMv Bdry Loss:  tensor(5.7926e+09)\n",
      "Iteration: 4800 \tTotal Loss: tensor(9.4068e+09)\n",
      "IC Loss:  tensor(12014.3086) \tBC Loss:  tensor(1.3898e-11) \tNS PDE Loss:  tensor(51.9939) \tMv Bdry Loss:  tensor(8.2002e+09)\n",
      "Iteration: 5000 \tTotal Loss: tensor(1.3682e+10)\n",
      "IC Loss:  tensor(12004.4492) \tBC Loss:  tensor(1.3414e-11) \tNS PDE Loss:  tensor(52.2028) \tMv Bdry Loss:  tensor(1.2476e+10)\n",
      "Iteration: 5200 \tTotal Loss: tensor(4.5577e+10)\n",
      "IC Loss:  tensor(11982.1484) \tBC Loss:  tensor(1.1827e-11) \tNS PDE Loss:  tensor(52.6524) \tMv Bdry Loss:  tensor(4.4373e+10)\n",
      "Iteration: 5400 \tTotal Loss: tensor(1.9500e+10)\n",
      "IC Loss:  tensor(12003.1396) \tBC Loss:  tensor(1.0640e-11) \tNS PDE Loss:  tensor(52.1692) \tMv Bdry Loss:  tensor(1.8295e+10)\n",
      "Iteration: 5600 \tTotal Loss: tensor(6.7221e+09)\n",
      "IC Loss:  tensor(11997.2207) \tBC Loss:  tensor(1.2588e-11) \tNS PDE Loss:  tensor(52.2407) \tMv Bdry Loss:  tensor(5.5171e+09)\n",
      "Iteration: 5800 \tTotal Loss: tensor(3.5355e+11)\n",
      "IC Loss:  tensor(11990.9277) \tBC Loss:  tensor(1.3148e-11) \tNS PDE Loss:  tensor(51.8888) \tMv Bdry Loss:  tensor(3.5234e+11)\n",
      "Iteration: 6000 \tTotal Loss: tensor(2.8508e+11)\n",
      "IC Loss:  tensor(12081.1836) \tBC Loss:  tensor(1.2793e-11) \tNS PDE Loss:  tensor(43.6815) \tMv Bdry Loss:  tensor(2.8387e+11)\n",
      "Iteration: 6200 \tTotal Loss: tensor(2.9712e+10)\n",
      "IC Loss:  tensor(12063.0693) \tBC Loss:  tensor(1.3270e-11) \tNS PDE Loss:  tensor(46.2790) \tMv Bdry Loss:  tensor(2.8501e+10)\n",
      "Iteration: 6400 \tTotal Loss: tensor(1.4189e+10)\n",
      "IC Loss:  tensor(12056.9473) \tBC Loss:  tensor(1.3200e-11) \tNS PDE Loss:  tensor(48.3505) \tMv Bdry Loss:  tensor(1.2979e+10)\n",
      "Iteration: 6600 \tTotal Loss: tensor(8.0923e+09)\n",
      "IC Loss:  tensor(12042.1621) \tBC Loss:  tensor(7.2982e-12) \tNS PDE Loss:  tensor(51.8812) \tMv Bdry Loss:  tensor(6.8829e+09)\n",
      "Iteration: 6800 \tTotal Loss: tensor(4.1822e+09)\n",
      "IC Loss:  tensor(12026.5488) \tBC Loss:  tensor(5.2714e-12) \tNS PDE Loss:  tensor(52.1040) \tMv Bdry Loss:  tensor(2.9744e+09)\n",
      "Iteration: 7000 \tTotal Loss: tensor(4.6204e+09)\n",
      "IC Loss:  tensor(12022.9746) \tBC Loss:  tensor(5.1346e-12) \tNS PDE Loss:  tensor(52.1199) \tMv Bdry Loss:  tensor(3.4129e+09)\n",
      "Iteration: 7200 \tTotal Loss: tensor(3.8284e+09)\n",
      "IC Loss:  tensor(12027.9932) \tBC Loss:  tensor(5.2819e-12) \tNS PDE Loss:  tensor(52.2065) \tMv Bdry Loss:  tensor(2.6204e+09)\n",
      "Iteration: 7400 \tTotal Loss: tensor(2.1101e+11)\n",
      "IC Loss:  tensor(11977.7100) \tBC Loss:  tensor(5.2380e-12) \tNS PDE Loss:  tensor(53.1188) \tMv Bdry Loss:  tensor(2.0981e+11)\n",
      "Iteration: 7600 \tTotal Loss: tensor(2.9625e+12)\n",
      "IC Loss:  tensor(12063.1660) \tBC Loss:  tensor(5.4743e-12) \tNS PDE Loss:  tensor(51.1443) \tMv Bdry Loss:  tensor(2.9613e+12)\n",
      "Iteration: 7800 \tTotal Loss: tensor(4.9354e+10)\n",
      "IC Loss:  tensor(12048.0361) \tBC Loss:  tensor(5.6436e-12) \tNS PDE Loss:  tensor(51.9743) \tMv Bdry Loss:  tensor(4.8144e+10)\n",
      "Iteration: 8000 \tTotal Loss: tensor(5.0251e+09)\n",
      "IC Loss:  tensor(12018.3984) \tBC Loss:  tensor(5.5195e-12) \tNS PDE Loss:  tensor(52.8225) \tMv Bdry Loss:  tensor(3.8180e+09)\n",
      "Iteration: 8200 \tTotal Loss: tensor(3.0061e+11)\n",
      "IC Loss:  tensor(12076.4912) \tBC Loss:  tensor(6.3711e-12) \tNS PDE Loss:  tensor(48.8873) \tMv Bdry Loss:  tensor(2.9940e+11)\n",
      "Iteration: 8400 \tTotal Loss: tensor(6.7096e+09)\n",
      "IC Loss:  tensor(12034.9814) \tBC Loss:  tensor(1.5555e-11) \tNS PDE Loss:  tensor(53.4258) \tMv Bdry Loss:  tensor(5.5007e+09)\n",
      "Iteration: 8600 \tTotal Loss: tensor(4.3456e+09)\n",
      "IC Loss:  tensor(11988.1660) \tBC Loss:  tensor(4.3923e-11) \tNS PDE Loss:  tensor(53.7206) \tMv Bdry Loss:  tensor(3.1414e+09)\n",
      "Iteration: 8800 \tTotal Loss: tensor(3.0247e+09)\n",
      "IC Loss:  tensor(12046.7998) \tBC Loss:  tensor(7.7159e-11) \tNS PDE Loss:  tensor(52.4924) \tMv Bdry Loss:  tensor(1.8148e+09)\n",
      "Iteration: 9000 \tTotal Loss: tensor(2.8632e+09)\n",
      "IC Loss:  tensor(12037.2197) \tBC Loss:  tensor(5.9568e-12) \tNS PDE Loss:  tensor(53.3259) \tMv Bdry Loss:  tensor(1.6541e+09)\n",
      "Iteration: 9200 \tTotal Loss: tensor(4.6715e+09)\n",
      "IC Loss:  tensor(12049.5293) \tBC Loss:  tensor(3.2154e-10) \tNS PDE Loss:  tensor(53.4842) \tMv Bdry Loss:  tensor(3.4612e+09)\n",
      "Iteration: 9400 \tTotal Loss: tensor(2.3988e+09)\n",
      "IC Loss:  tensor(12028.3848) \tBC Loss:  tensor(1.6240e-10) \tNS PDE Loss:  tensor(54.0175) \tMv Bdry Loss:  tensor(1.1906e+09)\n",
      "Iteration: 9600 \tTotal Loss: tensor(3.3667e+09)\n",
      "IC Loss:  tensor(11996.8506) \tBC Loss:  tensor(1.0298e-11) \tNS PDE Loss:  tensor(54.3498) \tMv Bdry Loss:  tensor(2.1616e+09)\n",
      "Iteration: 9800 \tTotal Loss: tensor(2.1697e+09)\n",
      "IC Loss:  tensor(12018.1553) \tBC Loss:  tensor(9.3219e-11) \tNS PDE Loss:  tensor(54.3530) \tMv Bdry Loss:  tensor(9.6247e+08)\n",
      "Iteration: 10000 \tTotal Loss: tensor(2.0646e+09)\n",
      "IC Loss:  tensor(12023.9492) \tBC Loss:  tensor(1.1289e-11) \tNS PDE Loss:  tensor(54.2825) \tMv Bdry Loss:  tensor(8.5673e+08)\n",
      "Iteration: 10200 \tTotal Loss: tensor(2.5052e+09)\n",
      "IC Loss:  tensor(12024.9814) \tBC Loss:  tensor(3.8000e-11) \tNS PDE Loss:  tensor(54.3711) \tMv Bdry Loss:  tensor(1.2973e+09)\n",
      "Iteration: 10400 \tTotal Loss: tensor(3.1559e+09)\n",
      "IC Loss:  tensor(12015.6689) \tBC Loss:  tensor(2.6055e-10) \tNS PDE Loss:  tensor(54.3346) \tMv Bdry Loss:  tensor(1.9489e+09)\n",
      "Iteration: 10600 \tTotal Loss: tensor(3.6052e+09)\n",
      "IC Loss:  tensor(12023.9297) \tBC Loss:  tensor(4.7618e-10) \tNS PDE Loss:  tensor(54.4095) \tMv Bdry Loss:  tensor(2.3974e+09)\n",
      "Iteration: 10800 \tTotal Loss: tensor(2.0291e+09)\n",
      "IC Loss:  tensor(12031.2607) \tBC Loss:  tensor(3.4415e-12) \tNS PDE Loss:  tensor(54.3277) \tMv Bdry Loss:  tensor(8.2056e+08)\n",
      "Iteration: 11000 \tTotal Loss: tensor(1.9926e+09)\n",
      "IC Loss:  tensor(12031.6338) \tBC Loss:  tensor(5.7938e-10) \tNS PDE Loss:  tensor(54.3379) \tMv Bdry Loss:  tensor(7.8401e+08)\n",
      "Iteration: 11200 \tTotal Loss: tensor(3.0384e+09)\n",
      "IC Loss:  tensor(12028.4385) \tBC Loss:  tensor(1.7343e-09) \tNS PDE Loss:  tensor(54.3662) \tMv Bdry Loss:  tensor(1.8301e+09)\n",
      "Iteration: 11400 \tTotal Loss: tensor(2.7069e+09)\n",
      "IC Loss:  tensor(12029.0244) \tBC Loss:  tensor(8.0536e-11) \tNS PDE Loss:  tensor(54.3152) \tMv Bdry Loss:  tensor(1.4986e+09)\n",
      "Iteration: 11600 \tTotal Loss: tensor(2.4605e+09)\n",
      "IC Loss:  tensor(12050.2324) \tBC Loss:  tensor(3.8545e-12) \tNS PDE Loss:  tensor(54.1993) \tMv Bdry Loss:  tensor(1.2500e+09)\n",
      "Iteration: 11800 \tTotal Loss: tensor(4.4185e+09)\n",
      "IC Loss:  tensor(12040.0244) \tBC Loss:  tensor(2.5824e-11) \tNS PDE Loss:  tensor(54.1343) \tMv Bdry Loss:  tensor(3.2091e+09)\n",
      "Iteration: 12000 \tTotal Loss: tensor(4.4981e+09)\n",
      "IC Loss:  tensor(12021.2520) \tBC Loss:  tensor(8.0863e-11) \tNS PDE Loss:  tensor(54.5100) \tMv Bdry Loss:  tensor(3.2905e+09)\n",
      "Iteration: 12200 \tTotal Loss: tensor(2.3035e+09)\n",
      "IC Loss:  tensor(12031.6240) \tBC Loss:  tensor(6.7315e-11) \tNS PDE Loss:  tensor(54.3220) \tMv Bdry Loss:  tensor(1.0949e+09)\n",
      "Iteration: 12400 \tTotal Loss: tensor(1.7808e+09)\n",
      "IC Loss:  tensor(12028.0889) \tBC Loss:  tensor(6.5282e-10) \tNS PDE Loss:  tensor(54.4779) \tMv Bdry Loss:  tensor(5.7259e+08)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "#Call model of layers and its forward step\n",
    "from Forward_with_Layer_Setting import Net\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Call training functions of Loss functions\n",
    "from NSpde_loss import lossNSpde\n",
    "from MvBdry_Coefficient_and_Loss import lossMvBdry \n",
    "from BoundaryLoss import lossBdry\n",
    "from InitialConditionLoss import lossIC, lossIC_with\n",
    "\n",
    "\n",
    "\n",
    "def create_network(IC_Only_Train):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "\n",
    "    #Set final times for running training\n",
    "    time_slices = np.array([.01,.1]) #, .25, .5, 1\n",
    "    \n",
    "    #Load Training Points\n",
    "    x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry = twoDimTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100))\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    #Start Training only on IC\n",
    "    if IC_Only_Train == True:\n",
    "        print('Training Only on the Initial Condition')\n",
    "        Create_IC_Parameters(x_domain, y_domain, t_zero, 50000, 10**-3, 'IC_Only.pt', record_loss = 100, print_loss = 1000)\n",
    "        IC_Done = time.time()\n",
    "        print('IC Time:\\t', IC_Done-start)\n",
    "        \n",
    "        print('Training Only on the NSpde Condition')\n",
    "        NSpde_Only_training(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, \n",
    "                      y_l_Bdry, y_u_Bdry, time_slices, 5000, 10**-3, record_loss = 100, print_loss = 1000)\n",
    "        torch.save(net.state_dict(), f\"NSpde_afterIC_.pt\")\n",
    "        NSpde_Done = time.time()\n",
    "        print('MvBdry Time:\\t', NSpde_Done-start)\n",
    "        \n",
    "        return 0\n",
    "        \n",
    "    time_vec = [0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    #attempt to load IC if it exists\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(\"IC_Only.pt\"))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #attempt to load MvBdry if it exists\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(\"NSpde_Only.pt\"))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    global epsilon #used to track loss\n",
    "    epsilon = []\n",
    "    \n",
    "    print('Training PDE')\n",
    "    \n",
    "    for i in range(4):\n",
    "        #Set loop to optimize in progressively smaller learning rates\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print('Executing Pass 1')\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-4    \n",
    "        elif i == 1:\n",
    "            print('Executing Pass 2')\n",
    "            #time_slices = time_slices[-1]\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-5\n",
    "        elif i == 2:\n",
    "            print('Executing Pass 3')\n",
    "            iterations = 20000\n",
    "            learning_rate = 5*10**-6\n",
    "        elif i ==3:\n",
    "            print('Executing Pass 4')\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-6\n",
    "        \n",
    "        training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, \n",
    "                      y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient = 1, record_loss = 100, print_loss = 200)\n",
    "        torch.save(net.state_dict(), f\"NNlayers_Bubble_{i}.pt\")\n",
    "        np.savetxt('epsilon.txt', epsilon)\n",
    "        time_vec[i] = time.time()\n",
    "\n",
    "    np.savetxt('epsilon.txt', epsilon)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Total Time:\\t\", end-start, '\\nPass 1 Time:\\t', time_vec[0]-start, '\\nPass 2 Time:\\t', time_vec[1]-start, '\\nPass 3 Time:\\t', time_vec[2]-start, '\\nPass 4 Time:\\t', time_vec[3]-start)\n",
    "\n",
    "\n",
    "def twoDimTrainPts(net, Domain_collocation, Bdry_collocation):\n",
    "    #Set of all the recorded xy variables as base data for chasing during training\n",
    "    \n",
    "    # Domain boundary in the range [0, 1]x[0, 2] and time in [0, 1].\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    y_l = net.x2_l\n",
    "    y_u = net.x2_u\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = 0\n",
    "\n",
    "    #Pick IC/Mv Bdry/NSpde Condition Training Random Points in Numpy\n",
    "    x_domain = np.random.uniform(low= x_l, high=x_u, size=(Domain_collocation, 1)) \n",
    "    y_domain = np.random.uniform(low= y_l, high=y_u, size=(Domain_collocation, 1)) \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "    y_domain = Variable(torch.from_numpy(y_domain).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(x_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    x_Bdry = np.random.uniform(low=x_l, high=x_u, size=(Bdry_collocation,1))\n",
    "    y_Bdry = np.random.uniform(low=y_l, high=y_u, size=(Bdry_collocation,1))       \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_Bdry= Variable(torch.from_numpy(x_Bdry).float(), requires_grad=True).to(device)\n",
    "    y_Bdry = Variable(torch.from_numpy(y_Bdry).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    ##Pick pts to make tensor for No-Slip Boundary Condition\n",
    "    x_l_Bdry = Variable(x_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    x_u_Bdry = Variable(x_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_l_Bdry = Variable(y_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_u_Bdry = Variable(y_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    \n",
    "            \n",
    "    return x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry\n",
    "    \n",
    "def tsliceTrainPts(net, Domain_collocation, Bdry_collocation, final_time):\n",
    "    #Set of all the recorded t variable as base data for chasing during training\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = net.t_l\n",
    "\n",
    "    #Pick IC/Mv Bdry/NSpde Condition Training Random Points in Numpy\n",
    "    t_domain = np.random.uniform(low=t_l, high=final_time, size=(Domain_collocation, 1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(t_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    t_Bdry = np.random.uniform(low=t_l, high=final_time, size=(Bdry_collocation,1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_Bdry = Variable(torch.from_numpy(t_Bdry).float(), requires_grad=True).to(device)\n",
    "        \n",
    "    return t_domain, t_Bdry\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def Create_IC_Parameters(x_domain, y_domain, t_zero, iterations, learning_rate, filename, record_loss, print_loss):\n",
    "    ICnet = Net().to(device)\n",
    "    \n",
    "    IC_Only_training(ICnet, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss)\n",
    "    \n",
    "    torch.save(ICnet.state_dict(), filename)\n",
    "    \n",
    "\n",
    "def IC_Only_training(net, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Domain boundary in the range [0, 1]x[0, 2] and time in [0, 1].\n",
    "    #x_l = net.x1_l\n",
    "    #x_u = net.x1_u\n",
    "    #y_l = net.x2_l\n",
    "    #y_u = net.x2_u\n",
    "\n",
    "    ##Define Colloacation Points with Initial Condition\n",
    "    #IC_collocation = collocation\n",
    "    \n",
    "    #define in numpy\n",
    "    #x_IC = np.random.uniform(low=x_l, high=x_u, size=(IC_collocation,1))\n",
    "    #y_IC = np.random.uniform(low=y_l, high=y_u, size=(IC_collocation,1))\n",
    "    \n",
    "    #move to pytorch tensors\n",
    "    #input_x_IC = Variable(torch.from_numpy(x_IC).float(), requires_grad=True).to(device)\n",
    "    #input_y_IC = Variable(torch.from_numpy(y_IC).float(), requires_grad=True).to(device)\n",
    "\n",
    "    \n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #training loop\n",
    "    epsilon_IC = [] #placeholder to track decreasing loss\n",
    "    for epoch in range(1, iterations+1):\n",
    "        \n",
    "    \n",
    "        # Resetting gradients to zero\n",
    "        net.optimizer.zero_grad()\n",
    "           \n",
    "        #Loss based on Initial Condition\n",
    "        loss = lossIC(net, x_domain, y_domain, t_zero)\n",
    "           \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Norm Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=2, error_if_nonfinite=True)\n",
    "\n",
    "        #Gradient Value Clipping\n",
    "        #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "        \n",
    "        net.optimizer.step()\n",
    "           \n",
    "        #Print Loss every 1000 Epochs\n",
    "        with torch.autograd.no_grad():\n",
    "            \n",
    "            if epoch%record_loss == 0:\n",
    "                epsilon_IC = np.append(epsilon_IC, loss.cpu().detach().numpy())\n",
    "            if epoch%print_loss == 0:\n",
    "                print(\"Iteration:\", epoch, \"Initial Condition Loss:\", loss.data)\n",
    "    \n",
    "    np.savetxt('epsilon_IC.txt', epsilon_IC)\n",
    "\n",
    "def NSpde_Only_training(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, record_loss, print_loss):\n",
    "    global epsilon\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        #indicator = False\n",
    "        reset_regularization = 1000\n",
    "        \n",
    "        #Iterate over these points\n",
    "        \n",
    "        t_domain, t_Bdry = tsliceTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100), final_time = final_time)    \n",
    "        for epoch in range(1, iterations+1):\n",
    "            # Loss calculation based on partial differential equation (PDE) \n",
    "            \n",
    "            if epoch%reset_regularization != 0: #To detect error on forward/Backward, add hashtag on this whole line, and\n",
    "            #with torch.autograd.detect_anomaly(): #use this line alternatively by deleting hashtag.\n",
    "                \n",
    "                ###Training steps\n",
    "                # Resetting gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "            \n",
    "                '''\n",
    "                #Loss based on Initial Condition\n",
    "                mse_IC = lossIC(net, x_domain, y_domain, t_zero)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC = lossBdry(net, x_Bdry, y_Bdry, t_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "                '''\n",
    "                #Loss based on PDE\n",
    "                mse_NS = lossNSpde(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "                '''\n",
    "            \n",
    "                #Loss based on Moving Boundary\n",
    "                mse_MvBdry = lossMvBdry(net, x_domain, y_domain, t_domain)\n",
    "                \n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                \n",
    "                if indicator == False:\n",
    "                    indicator = True\n",
    "                    IC_regular = mse_IC.detach()\n",
    "                    BC_regular = mse_BC.detach()\n",
    "                    pde_regular = mse_NS.detach()\n",
    "                    MvBdry_regular = mse_MvBdry.detach()\n",
    "                \n",
    "                raw_loss = mse_MvBdry\n",
    "                \n",
    "                mse_IC = mse_IC #/IC_regular\n",
    "                mse_BC = mse_BC #/BC_regular\n",
    "                mse_NS = mse_NS #/pde_regular\n",
    "                mse_MvBdry = mse_MvBdry #/MvBdry_regular\n",
    "                '''\n",
    "                #Combine all Loss functions\n",
    "                loss = 10**7 * mse_NS \n",
    "                # Gradient Norm Clipping\n",
    "                #torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                loss.backward()\n",
    "            \n",
    "            #Gradient Value Clipping\n",
    "            #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch%print_loss == 0:\n",
    "                    print(\"Iteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"\\tNspde Loss: \", mse_NS.data)\n",
    "                    \n",
    "\n",
    "def training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient, record_loss, print_loss):\n",
    "    global epsilon\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        indicator = False\n",
    "        reset_regularization = 1000\n",
    "        \n",
    "        #Iterate over these points\n",
    "        \n",
    "        t_domain, t_Bdry = tsliceTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100), final_time = final_time)    \n",
    "        for epoch in range(1, iterations+1):\n",
    "            # Loss calculation based on partial differential equation (PDE) \n",
    "            \n",
    "            if epoch%reset_regularization == 0:\n",
    "                indicator = False\n",
    "    \n",
    "            if epoch%reset_regularization != 0: #To detect error on forward/Backward, add hashtag on this whole line, and\n",
    "            #with torch.autograd.detect_anomaly(): #use this line alternatively by deleting hashtag.\n",
    "                \n",
    "                ###Training steps\n",
    "                # Resetting gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "            \n",
    "                #Loss based on Initial Condition\n",
    "                mse_IC = lossIC_with(net, x_domain, y_domain, t_zero)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC = lossBdry(net, x_Bdry, y_Bdry, t_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on PDE\n",
    "                mse_NS = lossNSpde(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            \n",
    "                #Loss based on Moving Boundary\n",
    "                mse_MvBdry = lossMvBdry(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            \n",
    "                if indicator == False:\n",
    "                    indicator = True\n",
    "                    IC_regular = mse_IC.detach()\n",
    "                    BC_regular = mse_BC.detach()\n",
    "                    pde_regular = mse_NS.detach()\n",
    "                    MvBdry_regular = mse_MvBdry.detach()\n",
    "                \n",
    "                raw_loss = IC_coefficient * mse_IC + mse_BC + mse_NS + mse_MvBdry\n",
    "            \n",
    "                mse_IC = mse_IC #/IC_regular\n",
    "                mse_BC = mse_BC #/BC_regular\n",
    "                mse_NS = mse_NS #/pde_regular\n",
    "                mse_MvBdry = mse_MvBdry #/MvBdry_regular\n",
    "            \n",
    "                #Combine all Loss functions\n",
    "                loss = mse_BC +mse_MvBdry + 10**5 *mse_IC  + 10**5 * mse_NS #IC_coefficient *\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                loss.backward()\n",
    "            # Gradient Norm Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            #Gradient Value Clipping\n",
    "            #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch%record_loss == 0:\n",
    "                    epsilon = np.append(epsilon, raw_loss.cpu().detach().numpy())\n",
    "                if epoch%print_loss == 0:\n",
    "                    print(\"Iteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"IC Loss: \", mse_IC.data, \"\\tBC Loss: \", mse_BC.data, \"\\tNS PDE Loss: \", mse_NS.data, \"\\tMv Bdry Loss: \", mse_MvBdry.data)\n",
    "\n",
    "            \n",
    "                \n",
    "create_network(True)\n",
    "create_network(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13182c8-069d-49cc-b89f-0a6289d603d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
