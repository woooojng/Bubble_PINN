{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f86f8-8eb5-43eb-b349-59e693608263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Only on the Initial Condition\n",
      "Iteration: 1000 Initial Condition Loss: tensor(0.0321)\n",
      "Iteration: 2000 Initial Condition Loss: tensor(0.0165)\n",
      "Iteration: 3000 Initial Condition Loss: tensor(0.0122)\n",
      "Iteration: 4000 Initial Condition Loss: tensor(0.0116)\n",
      "Iteration: 5000 Initial Condition Loss: tensor(0.0113)\n",
      "Iteration: 6000 Initial Condition Loss: tensor(0.0111)\n",
      "Iteration: 7000 Initial Condition Loss: tensor(0.0109)\n",
      "Iteration: 8000 Initial Condition Loss: tensor(0.0103)\n",
      "Iteration: 9000 Initial Condition Loss: tensor(0.0100)\n",
      "Iteration: 10000 Initial Condition Loss: tensor(0.0095)\n",
      "Iteration: 11000 Initial Condition Loss: tensor(0.0091)\n",
      "Iteration: 12000 Initial Condition Loss: tensor(0.0087)\n",
      "Iteration: 13000 Initial Condition Loss: tensor(0.0084)\n",
      "Iteration: 14000 Initial Condition Loss: tensor(0.0082)\n",
      "Iteration: 15000 Initial Condition Loss: tensor(0.0077)\n",
      "Iteration: 16000 Initial Condition Loss: tensor(0.0077)\n",
      "Iteration: 17000 Initial Condition Loss: tensor(0.0075)\n",
      "Iteration: 18000 Initial Condition Loss: tensor(0.0075)\n",
      "Iteration: 19000 Initial Condition Loss: tensor(0.0071)\n",
      "Iteration: 20000 Initial Condition Loss: tensor(0.0068)\n",
      "Iteration: 21000 Initial Condition Loss: tensor(0.0069)\n",
      "Iteration: 22000 Initial Condition Loss: tensor(0.0064)\n",
      "Iteration: 23000 Initial Condition Loss: tensor(0.0060)\n",
      "Iteration: 24000 Initial Condition Loss: tensor(0.0059)\n",
      "Iteration: 25000 Initial Condition Loss: tensor(0.0056)\n",
      "Iteration: 26000 Initial Condition Loss: tensor(0.0054)\n",
      "Iteration: 27000 Initial Condition Loss: tensor(0.0052)\n",
      "Iteration: 28000 Initial Condition Loss: tensor(0.0050)\n",
      "Iteration: 29000 Initial Condition Loss: tensor(0.0046)\n",
      "Iteration: 30000 Initial Condition Loss: tensor(0.0045)\n",
      "IC Time:\t 208.01424288749695\n",
      "Training PDE\n",
      "Executing Pass 1\n",
      "Current Final Time: 0.1 Current Learning Rate:  0.01\n",
      "Iteration: 200 \tTotal Loss: tensor(1.5539e+13)\n",
      "IC Loss:  tensor(352.4767) \tBC Loss:  tensor(1690.3413) \tNS PDE Loss:  tensor(1.5539e+08) \tNS Div Free Loss:  tensor(1.0200e-09)\n",
      "Iteration: 400 \tTotal Loss: tensor(9.4130e+12)\n",
      "IC Loss:  tensor(1031.7177) \tBC Loss:  tensor(2920.6860) \tNS PDE Loss:  tensor(94128784.) \tNS Div Free Loss:  tensor(4.9989e-09)\n",
      "Iteration: 600 \tTotal Loss: tensor(1.4309e+13)\n",
      "IC Loss:  tensor(729.1233) \tBC Loss:  tensor(2934.8630) \tNS PDE Loss:  tensor(1.4309e+08) \tNS Div Free Loss:  tensor(2.4725e-09)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "\n",
    "#Call model of layers and its forward step\n",
    "from Forward_with_Layer_Setting import Net\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Call training functions of Loss functions\n",
    "from NSpde_loss import lossNSpde, DivLoss\n",
    "from BoundaryLoss import lossBdry\n",
    "from InitialConditionLoss import lossIC\n",
    "\n",
    "\n",
    "\n",
    "def create_network(IC_Only_Train):\n",
    "    \n",
    "    net = Net()\n",
    "    net = net.to(device)\n",
    "\n",
    "    #Set final times for running training\n",
    "    time_slices = np.array([.1,.2]) #, .3, .4, .5, .6, .7, .8, .9, 1\n",
    "    \n",
    "    #Load Training Points\n",
    "    x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry = twoDimTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100))\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    #Start Training only on IC\n",
    "    if IC_Only_Train == True:\n",
    "        print('Training Only on the Initial Condition')\n",
    "        Create_IC_Parameters(x_domain, y_domain, t_zero, 30000, 10**-3, 'IC_Only.pt', record_loss = 100, print_loss = 1000)\n",
    "        IC_Done = time.time()\n",
    "        print('IC Time:\\t', IC_Done-start)\n",
    "        \n",
    "        return 0\n",
    "        \n",
    "    time_vec = [0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    #attempt to load IC if it exists\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(\"IC_Only.pt\"))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    global epsilon #used to track loss\n",
    "    epsilon = []\n",
    "    \n",
    "    print('Training PDE')\n",
    "    \n",
    "    for i in range(4):\n",
    "        #Set loop to optimize in progressively smaller learning rates\n",
    "        if i == 0:\n",
    "            #First loop uses progressively increasing time intervals\n",
    "            print('Executing Pass 1')\n",
    "            iterations = 20000\n",
    "            learning_rate = 10**-2    \n",
    "        elif i == 1:\n",
    "            print('Executing Pass 2')\n",
    "            #time_slices = time_slices[-1]\n",
    "            iterations = 2 #0000\n",
    "            learning_rate = 10**-3\n",
    "        elif i == 2:\n",
    "            print('Executing Pass 3')\n",
    "            iterations = 2 #0000\n",
    "            learning_rate = 5*10**-6\n",
    "        elif i ==3:\n",
    "            print('Executing Pass 4')\n",
    "            iterations = 2 #0000\n",
    "            learning_rate = 10**-6\n",
    "        \n",
    "        training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, \n",
    "                      y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient = 1, record_loss = 100, print_loss = 200)\n",
    "        torch.save(net.state_dict(), f\"NNlayers_Bubble_{i}.pt\")\n",
    "        np.savetxt('epsilon.txt', epsilon)\n",
    "        time_vec[i] = time.time()\n",
    "\n",
    "    np.savetxt('epsilon.txt', epsilon)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Total Time:\\t\", end-start, '\\nPass 1 Time:\\t', time_vec[0]-start, '\\nPass 2 Time:\\t', time_vec[1]-start, '\\nPass 3 Time:\\t', time_vec[2]-start, '\\nPass 4 Time:\\t', time_vec[3]-start)\n",
    "\n",
    "\n",
    "def twoDimTrainPts(net, Domain_collocation, Bdry_collocation):\n",
    "    #Set of all the recorded xy variables as base data for chasing during training\n",
    "    \n",
    "    # Domain boundary in the range [0, 1]x[0, 2] and time in [0, 1].\n",
    "    x_l = net.x1_l\n",
    "    x_u = net.x1_u\n",
    "    y_l = net.x2_l\n",
    "    y_u = net.x2_u\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = 0\n",
    "\n",
    "    #Pick IC/NSpde Condition Training Random Points in Numpy\n",
    "    x_domain = np.random.uniform(low= x_l, high=x_u, size=(Domain_collocation, 1)) \n",
    "    y_domain = np.random.uniform(low= y_l, high=y_u, size=(Domain_collocation, 1)) \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_domain = Variable(torch.from_numpy(x_domain).float(), requires_grad=True).to(device)\n",
    "    y_domain = Variable(torch.from_numpy(y_domain).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(x_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    x_Bdry = np.random.uniform(low=x_l, high=x_u, size=(Bdry_collocation,1))\n",
    "    y_Bdry = np.random.uniform(low=y_l, high=y_u, size=(Bdry_collocation,1))       \n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    x_Bdry= Variable(torch.from_numpy(x_Bdry).float(), requires_grad=True).to(device)\n",
    "    y_Bdry = Variable(torch.from_numpy(y_Bdry).float(), requires_grad=True).to(device)\n",
    "    \n",
    "    ##Pick pts to make tensor for No-Slip Boundary Condition\n",
    "    x_l_Bdry = Variable(x_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    x_u_Bdry = Variable(x_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_l_Bdry = Variable(y_l * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    y_u_Bdry = Variable(y_u * torch.ones_like(x_Bdry), requires_grad=True).to(device)\n",
    "    \n",
    "            \n",
    "    return x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry\n",
    "    \n",
    "def tsliceTrainPts(net, Domain_collocation, Bdry_collocation, final_time):\n",
    "    #Set of all the recorded t variable as base data for chasing during training\n",
    "\n",
    "    #time starts at lower bound 0, ends at upper bouund updated in slices\n",
    "    t_l = net.t_l\n",
    "\n",
    "    #Pick IC/NSpde Condition Training Random Points in Numpy\n",
    "    t_domain = np.random.uniform(low=t_l, high=final_time, size=(Domain_collocation, 1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_domain = Variable(torch.from_numpy(t_domain).float(), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick IC Training t starting points to make tensor\n",
    "    t_zero = Variable(torch.zeros_like(t_domain), requires_grad=True).to(device)\n",
    "\n",
    "    #Pick BC Training Random Points in Numpy\n",
    "    t_Bdry = np.random.uniform(low=t_l, high=final_time, size=(Bdry_collocation,1))\n",
    "    \n",
    "    #Move to pytorch tensors\n",
    "    t_Bdry = Variable(torch.from_numpy(t_Bdry).float(), requires_grad=True).to(device)\n",
    "        \n",
    "    return t_domain, t_Bdry\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def Create_IC_Parameters(x_domain, y_domain, t_zero, iterations, learning_rate, filename, record_loss, print_loss):\n",
    "    ICnet = Net().to(device)\n",
    "    \n",
    "    IC_Only_training(ICnet, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss)\n",
    "    \n",
    "    torch.save(ICnet.state_dict(), filename)\n",
    "    \n",
    "\n",
    "def IC_Only_training(net, x_domain, y_domain, t_zero, iterations, learning_rate, record_loss, print_loss):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    #training loop\n",
    "    epsilon_IC = [] #placeholder to track decreasing loss\n",
    "    for epoch in range(1, iterations+1):\n",
    "        \n",
    "    \n",
    "        # Resetting gradients to zero\n",
    "        net.optimizer.zero_grad()\n",
    "           \n",
    "        #Loss based on Initial Condition\n",
    "        loss = lossIC(net, x_domain, y_domain, t_zero)\n",
    "           \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Norm Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=2, error_if_nonfinite=True)\n",
    "\n",
    "        #Gradient Value Clipping\n",
    "        #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "        \n",
    "        net.optimizer.step()\n",
    "           \n",
    "        #Print Loss every 1000 Epochs\n",
    "        with torch.autograd.no_grad():\n",
    "            \n",
    "            if epoch%record_loss == 0:\n",
    "                epsilon_IC = np.append(epsilon_IC, loss.cpu().detach().numpy())\n",
    "            if epoch%print_loss == 0:\n",
    "                print(\"Iteration:\", epoch, \"Initial Condition Loss:\", loss.data)\n",
    "    \n",
    "    np.savetxt('epsilon_IC.txt', epsilon_IC)\n",
    "\n",
    "\n",
    "def training_loop(net, x_domain, y_domain, t_zero, x_Bdry, y_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry, time_slices, iterations, learning_rate, IC_coefficient, record_loss, print_loss):\n",
    "    global epsilon\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate update\n",
    "    for g in net.optimizer.param_groups:\n",
    "        g['lr'] = learning_rate\n",
    "    \n",
    "    for final_time in time_slices:\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            print(\"Current Final Time:\", final_time, \"Current Learning Rate: \", get_lr(net.optimizer))  \n",
    "        \n",
    "        indicator = False\n",
    "        reset_regularization = 1000\n",
    "        \n",
    "        #Iterate over these points\n",
    "        \n",
    "        t_domain, t_Bdry = tsliceTrainPts(net, Domain_collocation = int(1000), Bdry_collocation = int(100), final_time = final_time)    \n",
    "        for epoch in range(1, iterations+1):\n",
    "            # Loss calculation based on partial differential equation (PDE) \n",
    "            \n",
    "            if epoch%reset_regularization == 0:\n",
    "                indicator = False\n",
    "    \n",
    "            if epoch%reset_regularization != 0: #To detect error on forward/Backward, add hashtag on this whole line, and\n",
    "            #with torch.autograd.detect_anomaly(): #use this line alternatively by deleting hashtag.\n",
    "                \n",
    "                ###Training steps\n",
    "                # Resetting gradients to zero\n",
    "                net.optimizer.zero_grad()\n",
    "            \n",
    "                #Loss based on Initial Condition\n",
    "                mse_IC = lossIC(net, x_domain, y_domain, t_zero)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on Boundary Condition (Containing No-Slip and Free-slip)\n",
    "                mse_BC = lossBdry(net, x_Bdry, y_Bdry, t_Bdry, x_l_Bdry, x_u_Bdry, y_l_Bdry, y_u_Bdry)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                #Loss based on PDE\n",
    "                mse_NS = lossNSpde(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "                \n",
    "                mse_NSdiv = DivLoss(net, x_domain, y_domain, t_domain)\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            \n",
    "                if indicator == False:\n",
    "                    indicator = True\n",
    "                    IC_regular = mse_IC.detach()\n",
    "                    BC_regular = mse_BC.detach()\n",
    "                    pde_regular = mse_NS.detach()\n",
    "                    pdediv_regular = mse_NSdiv.detach()\n",
    "                \n",
    "                raw_loss = IC_coefficient * mse_IC + mse_BC + mse_NS + mse_NSdiv\n",
    "            \n",
    "                mse_IC = mse_IC #/IC_regular\n",
    "                mse_BC = mse_BC #/BC_regular\n",
    "                mse_NS = mse_NS #/pde_regular\n",
    "                mse_NSdiv = mse_NSdiv #/pdediv_regular\n",
    "            \n",
    "                #Combine all Loss functions\n",
    "                loss = mse_BC + 10**5 *mse_IC  + 10**5 * mse_NS + 10**5 * mse_NSdiv #IC_coefficient *\n",
    "                # Gradient Norm Clipping\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "                loss.backward()\n",
    "            # Gradient Norm Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm= 5*10**2, norm_type=1, error_if_nonfinite=False)\n",
    "\n",
    "            #Gradient Value Clipping\n",
    "            #nn.utils.clip_grad_value_(net.parameters(), clip_value=1.0)\n",
    "            net.optimizer.step()\n",
    "            \n",
    "            #Print Loss every 1000 Epochs\n",
    "            with torch.autograd.no_grad():\n",
    "                if epoch%record_loss == 0:\n",
    "                    epsilon = np.append(epsilon, raw_loss.cpu().detach().numpy())\n",
    "                if epoch%print_loss == 0:\n",
    "                    print(\"Iteration:\", epoch, \"\\tTotal Loss:\", loss.data)\n",
    "                    print(\"IC Loss: \", mse_IC.data, \"\\tBC Loss: \", mse_BC.data, \"\\tNS PDE Loss: \", mse_NS.data, \"\\tNS Div Free Loss: \", mse_NSdiv.data)\n",
    "\n",
    "            \n",
    "                \n",
    "create_network(True)\n",
    "create_network(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be064db9-312a-4337-9f2e-03e27fe51ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ce6a2-7126-4467-97b8-dec6b6087cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
